{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178800e9-18a7-4409-bf8a-87be8ff484ba",
   "metadata": {},
   "source": [
    "# Power & Language - Topic Model experiments\n",
    "\n",
    "- [Data @OSF](https://osf.io/dwnxt/?view_only=e75faa4f54244361aa198e257b4fecf9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fbe6d1-7f0c-44fb-b9e6-cec1127d5aa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LWIC (write and load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0b8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_lwic = \"LIWC_German/LIWC_German.dic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c3a8ac-70dd-40e8-ba81-f73d4c2cbd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_lwic(fn):\n",
    "    in_header = True\n",
    "    categories = dict()\n",
    "    word2cats = dict()\n",
    "    cat2words = dict()\n",
    "    with open(fn, \"r\") as fp:\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            if line.startswith(\"%\"):\n",
    "                in_header = False\n",
    "                continue\n",
    "\n",
    "            if in_header:\n",
    "                num, name = line.rstrip().split(\"\\t\")\n",
    "                categories[int(num)] = name\n",
    "\n",
    "            else:\n",
    "                word, *cats = line.rstrip().split(\"\\t\")\n",
    "                cats = list(map(int, cats))\n",
    "                word2cats[word] = cats\n",
    "                for cat in cats:\n",
    "                    try:\n",
    "                        cat2words[cat].append(word)\n",
    "                    except KeyError:\n",
    "                        cat2words[cat] = [word]\n",
    "\n",
    "    return categories, word2cats, cat2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02acf614-c3c1-42ec-ba9f-c8b45c1b7753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Pronoun',\n",
       " 2: 'I',\n",
       " 3: 'We',\n",
       " 4: 'Self',\n",
       " 5: 'You',\n",
       " 6: 'Other',\n",
       " 7: 'Negate',\n",
       " 8: 'Assent',\n",
       " 9: 'Article',\n",
       " 10: 'Preps',\n",
       " 11: 'Numbers',\n",
       " 12: 'Affect',\n",
       " 13: 'Positiveemotion',\n",
       " 14: 'Positivefeeling',\n",
       " 15: 'Optimism',\n",
       " 16: 'Negativeemotion',\n",
       " 17: 'Anxiety',\n",
       " 18: 'Anger',\n",
       " 19: 'Sad',\n",
       " 20: 'Cognitivemechanism',\n",
       " 21: 'Cause',\n",
       " 22: 'Insight',\n",
       " 23: 'Discrepancy',\n",
       " 24: 'Inhibition',\n",
       " 25: 'Tentative',\n",
       " 26: 'Certain',\n",
       " 31: 'Social',\n",
       " 32: 'Communication',\n",
       " 33: 'Otherreference',\n",
       " 34: 'Friends',\n",
       " 35: 'Family',\n",
       " 36: 'Humans',\n",
       " 37: 'Time',\n",
       " 38: 'Past',\n",
       " 39: 'Present',\n",
       " 40: 'Future',\n",
       " 41: 'Space',\n",
       " 42: 'Up',\n",
       " 43: 'Down',\n",
       " 44: 'Incl',\n",
       " 45: 'Excl',\n",
       " 46: 'Motion',\n",
       " 47: 'Occup',\n",
       " 48: 'School',\n",
       " 49: 'Job',\n",
       " 50: 'Achieve',\n",
       " 51: 'Leisure',\n",
       " 52: 'Home',\n",
       " 53: 'Sports',\n",
       " 54: 'TV',\n",
       " 55: 'Music',\n",
       " 56: 'Money',\n",
       " 57: 'Metaph',\n",
       " 58: 'Relig',\n",
       " 59: 'Death',\n",
       " 60: 'Physical',\n",
       " 61: 'Body',\n",
       " 62: 'Sex',\n",
       " 63: 'Eat',\n",
       " 64: 'Sleep',\n",
       " 65: 'Grooming',\n",
       " 66: 'Swear',\n",
       " 67: 'Nonfluency',\n",
       " 68: 'Fillers'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories, word2cats, cat2words = load_lwic(fn_lwic)\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181ece3-4f13-4d46-98b2-d78cbeaccb7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c29a13-c568-4a1d-ab03-02453c9b96e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6433d5-0b7f-486c-a2ba-bb8cb0789117",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c387a37e-6ca0-464e-b7fe-a57625caddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fasttext.util\n",
    "#fasttext.util.download_model('de') #, if_exists='ignore')  # German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e67c-a3a7-457b-88a2-8fed03c459d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python -m spacy download de_core_news_sm\n",
    "#! python -m spacy download de_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a212b92-7353-4a42-ad71-2538dfdf0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9337473-5350-461c-ab72-df07b6e6f340",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Env Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cb7ef-fd2a-4f3a-80de-ef21be1ac322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c60892-2610-4c2c-b43f-06a5466dbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "    \n",
    "import de_core_news_sm\n",
    "#import de_dep_news_trf\n",
    "\n",
    "nlp = de_core_news_sm.load()\n",
    "#nlp = de_dep_news_trf.load()\n",
    "\n",
    "#nlp = spacy.load(\"de_core_news_sm\")\n",
    "#nlp = spacy.load(\"de_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35627f0e-27d0-4133-9e3e-21737e72688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "ft = fasttext.load_model('cc.de.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46905f25-7c22-4ff2-870c-2646dd831a0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load study data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6879895a-add9-46b0-a3cc-c795a808908d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_study1():\n",
    "    df_study1 = pd.read_excel(\"Study 1/Data Study 1.xlsx\")\n",
    "    #print(df_study1.columns.to_list())\n",
    "\n",
    "    # just keep useful columns\n",
    "    df_study1 = df_study1[[\n",
    "        # id\n",
    "        \"ID\",\n",
    "        # raw text\n",
    "        \"SourceB\",\n",
    "        # other meta\n",
    "        \"Alter\", \"Geschlecht\",\n",
    "        # self-evaluation (mean)\n",
    "        \"Power_mean\", \"Dom_mean\", \"Pres_mean\",\n",
    "        # outside-evaluation (mean)\n",
    "        \"Power_F\", \"Dom_F\", \"Pres_F\"\n",
    "    ]]\n",
    "\n",
    "    # rename columns\n",
    "    df_study1.rename(columns={\n",
    "        \"SourceB\": \"text\", \"Alter\": \"age\", \"Geschlecht\": \"gender\",\n",
    "        \"Power_mean\": \"power\", \"Dom_mean\": \"dominance\", \"Pres_mean\": \"prestige\",\n",
    "        \"Power_F\": \"power_f\", \"Dom_F\": \"dominance_f\", \"Pres_F\": \"prestige_f\",\n",
    "    }, inplace=True)\n",
    "    \n",
    "    return df_study1\n",
    "\n",
    "\n",
    "def load_study2():\n",
    "    df_study2 = pd.read_excel(\"Study 2/Data Study 2.xlsx\")\n",
    "    #print(df_study2.columns.to_list())\n",
    "\n",
    "    # just keep useful columns\n",
    "    df_study2 = df_study2[[\n",
    "        # id\n",
    "        \"ID\",\n",
    "        # raw text\n",
    "        \"SourceA\",\n",
    "        # other meta\n",
    "        \"Alter\", \"Geschlecht\",\n",
    "        # self-evaluation (mean)\n",
    "        \"Power_means\", \"Dominanz_means\", \"Prestige_means\",\n",
    "        # outside-evaluation (mean)\n",
    "        \"Power_Fremdgesamt_means\", \"Dominanz_Fremdgesamt_means\", \"Prestige_Fremdgesamt_means\",\n",
    "        # WP?\n",
    "        #\"WP_means\",, \"WP_Fremdgesamt_means\",\n",
    "    ]]\n",
    "\n",
    "    # rename columns\n",
    "    df_study2.rename(columns={\n",
    "        \"SourceA\": \"text\", \"Alter\": \"age\", \"Geschlecht\": \"gender\",\n",
    "        \"Power_means\": \"power\", \"Dominanz_means\": \"dominance\", \"Prestige_means\": \"prestige\",\n",
    "        \"Power_Fremdgesamt_means\": \"power_f\", \"Dominanz_Fremdgesamt_means\": \"dominance_f\", \"Prestige_Fremdgesamt_means\": \"prestige_f\",\n",
    "        \"WP_means\": \"wp\", \"WP_Fremdgesamt_means\": \"wp_f\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df_study2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d31365-6530-4d0f-a008-53d53ebc483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_study1 = load_study1()\n",
    "#df_study1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89a686-2731-43c9-b7c5-78faf95d13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_study2 = load_study2()\n",
    "#df_study2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2673e-d1f8-4e3b-a756-543125e7976e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cleanup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b712b1e7-4822-437b-b2d1-23dea87fddd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nlpize(df, nlp_fn):\n",
    "    return df.map(nlp_fn)\n",
    "\n",
    "\n",
    "def clean(df, stopwords=False, alpha=False, punctuation=True):\n",
    "    # filter out stopwords\n",
    "    if stopwords:\n",
    "        df = df.map(lambda doc: list(filter(lambda tok: not tok.is_stop, doc)))\n",
    "\n",
    "    # filter alphanumerical\n",
    "    if alpha:\n",
    "        df = df.map(lambda doc: list(filter(lambda tok: tok.is_alpha, doc)))\n",
    "\n",
    "    # filter out punctuation\n",
    "    if punctuation:\n",
    "        df = df.map(lambda doc: list(filter(lambda tok: tok.pos_ not in (\"PUNCT\"), doc)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punct(df):\n",
    "    return df.map(lambda x: re.sub(r\"[,\\.!?]\", '', x))\n",
    "\n",
    "\n",
    "def lowercase_text(df):\n",
    "    return df.map(lambda x: x.lower())\n",
    "\n",
    "\n",
    "def get_text_by_pos(df, pos_list=(\"NOUN\",), lemma=False, join=True):\n",
    "    # filter each token by correct pos tag\n",
    "    if pos_list:\n",
    "      df = df.map(lambda x: list(filter(lambda tok: tok.pos_ in pos_list, x)))\n",
    "\n",
    "    # convert tokens back to strings\n",
    "    #df = df.map(lambda x: \" \".join(map(str, x)))\n",
    "    if lemma:\n",
    "        df = df.map(lambda x: \" \".join(map(lambda tok: tok.lemma_, x)))\n",
    "    else:\n",
    "        df = df.map(lambda x: \" \".join(map(lambda tok: tok.text, x)))\n",
    "    \n",
    "    # concat to single text\n",
    "    if not join:\n",
    "        return df\n",
    "\n",
    "    return ','.join(df.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fcd50-faf5-4b84-988e-fff33f6f86d3",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a9f32-840d-4912-9433-62c6334cb931",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d4d091-6a05-4625-af2a-f3e2b70f6248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_study1 = load_study1()\n",
    "df_study2 = load_study2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331663db-fd29-49e1-9375-e639dff464f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 0 ns, total: 10.9 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%%script false --no-raise-error\n",
    "# tokenize, postag, ...\n",
    "df_study1[\"text_spacy_doc\"] = nlpize(df_study1[\"text\"], nlp)\n",
    "df_study2[\"text_spacy_doc\"] = nlpize(df_study2[\"text\"], nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6c8525-f48c-41d9-b95e-d3d6e16705d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 ms, sys: 0 ns, total: 47.3 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove punctuation\n",
    "do_clean_stopwords = True\n",
    "df_study1[\"text_spacy_doc_filtered\"] = clean(df_study1[\"text_spacy_doc\"], stopwords=do_clean_stopwords, alpha=False, punctuation=True)\n",
    "df_study2[\"text_spacy_doc_filtered\"] = clean(df_study2[\"text_spacy_doc\"], stopwords=do_clean_stopwords, alpha=False, punctuation=True)\n",
    "\n",
    "# take raw text `tok.text` instead of lemma `tok.lemma_`\n",
    "df_study1[\"tokens\"] = df_study1[\"text_spacy_doc_filtered\"].map(lambda doc: list(map(lambda tok: tok.text, doc)))\n",
    "df_study2[\"tokens\"] = df_study2[\"text_spacy_doc_filtered\"].map(lambda doc: list(map(lambda tok: tok.text, doc)))\n",
    "# convert to plain string\n",
    "df_study1[\"tokens\"] = df_study1[\"tokens\"].map(lambda doc: list(map(str, doc)))\n",
    "df_study2[\"tokens\"] = df_study2[\"tokens\"].map(lambda doc: list(map(str, doc)))\n",
    "\n",
    "# concat both studies\n",
    "#docs = pd.concat([df_study1[\"tokens\"], df_study2[\"tokens\"]], ignore_index=True)\n",
    "#docs_raw = docs.map(lambda doc: \" \".join(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cd340-9f35-4a93-8063-94dc2229c9ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate lwic cluster embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c22b8756-a1c9-4937-a521-5aec079d76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, word2cats, cat2words = load_lwic(fn_lwic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d9d0a00-03b0-4a5e-a169-866c1f170c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.1 s, sys: 0 ns, total: 38.1 s\n",
      "Wall time: 38.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate mean vectors based on category words\n",
    "cat2emb = dict()\n",
    "\n",
    "for cat, words in cat2words.items():\n",
    "    vectors = list()\n",
    "    weights = [1.0] * len(words)\n",
    "    for word, weight in zip(words, weights):\n",
    "        if word.endswith(\"*\"):\n",
    "            word = word[:-1]\n",
    "        emb = ft.get_word_vector(word)\n",
    "        #vectors.append([word, weight, emb])\n",
    "        vectors.append(emb)\n",
    "    cat_emb = np.array(vectors).mean(axis=0, dtype=np.float64)\n",
    "    cat2emb[cat] = cat_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "642e165d-bbaf-41cb-bef6-477d80a08576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import matutils\n",
    "\n",
    "\n",
    "def compute_most_similar(vector_1, vectors_all, topn=10):\n",
    "    \"\"\"Compute most similar for `vector_1` to `vectors_all`. Normalizes the\n",
    "    vectors and computes the cosine (dot product).\n",
    "    Return the `topn` best matches.\"\"\"\n",
    "    # from most_similar(..) L491 (Word2Vec gensim)\n",
    "    # assumes vectors_all is normalized\n",
    "    # mean = matutils.unitvec(np.array([vector_1]).mean(axis=0)).astype(np.float32)\n",
    "    # distances = np.dot(vectors_all[0:len(vectors_all)], mean)\n",
    "\n",
    "    # from cosine_similarities(..) L883 (Word2Vec gensim)\n",
    "    # computes norms on the fly and normalizes (L2) both inputs\n",
    "    norm = np.linalg.norm(vector_1)\n",
    "    all_norms = np.linalg.norm(vectors_all, axis=1)\n",
    "    dot_products = np.dot(vectors_all, vector_1)\n",
    "    distances = dot_products / (norm * all_norms)\n",
    "\n",
    "    # from most_similar\n",
    "    best = matutils.argsort(distances, topn=topn + len(vectors_all), reverse=True)\n",
    "    result = [(sim_idx, float(distances[sim_idx])) for sim_idx in best]\n",
    "    return result[:topn]\n",
    "\n",
    "\n",
    "def compute_most_similar_labeled_base(vector, vectors_all, labels, topn=10):\n",
    "    \"\"\"Transform most similar result from indices to vector labels\n",
    "    (i. e. words).\"\"\"\n",
    "    results = compute_most_similar(vector, vectors_all, topn=topn)\n",
    "    results = [(labels[i], sim) for i, sim in results]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e53dcae-a788-4088-8281-506e36fd971e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "num_vec_sims = 5\n",
    "cat_labels, cat_embs = zip(*cat2emb.items())\n",
    "\n",
    "num_found = num_inferred = 0\n",
    "\n",
    "def map_tokens2category(tokens):\n",
    "    global num_found, num_inferred\n",
    "    label_cnt = Counter()\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word in word2cats:\n",
    "            label_cnt.update(word2cats[word])\n",
    "            num_found += 1\n",
    "            continue\n",
    "\n",
    "        emb = ft.get_word_vector(word)\n",
    "        num_inferred += 1\n",
    "        labels = compute_most_similar_labeled_base(emb, list(cat_embs), cat_labels, topn=num_vec_sims)\n",
    "        label_cnt.update([l for l, _ in labels])\n",
    "\n",
    "    top_labels = label_cnt.most_common(10)\n",
    "    top_labels = sorted(top_labels, key=lambda x: (x[1], x[0]))\n",
    "    top_labels = [(categories[ln], cnt) for ln, cnt in top_labels]\n",
    "    #top_labels = [ln for ln, _ in top_labels]\n",
    "    return top_labels\n",
    "\n",
    "\n",
    "#map_tokens2category(df_study1[\"tokens\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e651f2e4-341b-4e81-a212-e215065a9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1670/2367861747.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  distances = dot_products / (norm * all_norms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study1: num_found=633, num_inferred=3746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1670/2367861747.py:19: RuntimeWarning: invalid value encountered in divide\n",
      "  distances = dot_products / (norm * all_norms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study2: num_found=560, num_inferred=4886\n",
      "CPU times: user 59.9 s, sys: 8min 37s, total: 9min 37s\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [Anxiety, Time, Cognitivemechanism, Excl, Opti...\n",
       "1      [Anxiety, Friends, Metaph, Optimism, Job, Affe...\n",
       "2      [Sex, Family, Present, School, Job, Leisure, T...\n",
       "3      [Positiveemotion, Positivefeeling, Cognitiveme...\n",
       "4      [Death, Positivefeeling, Cognitivemechanism, C...\n",
       "                             ...                        \n",
       "195    [Positivefeeling, Friends, Occup, School, Leis...\n",
       "196    [Humans, Positivefeeling, Cognitivemechanism, ...\n",
       "197    [Anger, Metaph, Negativeemotion, Certain, Occu...\n",
       "198    [Preps, Occup, School, Affect, Positivefeeling...\n",
       "199    [Preps, Affect, Positiveemotion, Anxiety, TV, ...\n",
       "Name: cats_plain, Length: 200, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "num_found = num_inferred = 0  # reset counter\n",
    "\n",
    "df_study1[\"cats\"] = df_study1[\"tokens\"].map(map_tokens2category)\n",
    "df_study1[\"cats_plain\"] = df_study1[\"cats\"].map(lambda x: [n for n, _ in x])\n",
    "print(f\"study1: {num_found=}, {num_inferred=}\")\n",
    "num_found = num_inferred = 0\n",
    "\n",
    "df_study2[\"cats\"] = df_study2[\"tokens\"].map(map_tokens2category)\n",
    "df_study2[\"cats_plain\"] = df_study2[\"cats\"].map(lambda x: [n for n, _ in x])\n",
    "print(f\"study2: {num_found=}, {num_inferred=}\")\n",
    "num_found = num_inferred = 0\n",
    "\n",
    "df_study1[\"cats_plain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0d79a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          Ich würde mich selbst als fleißig, disziplinie...\n",
       "cats_plain    [Anxiety, Time, Cognitivemechanism, Excl, Opti...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_study1[[\"text\", \"cats_plain\"]].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7ab11-77ed-4847-837f-19bd719eea6f",
   "metadata": {},
   "source": [
    "### Cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "975b65c1-af05-4fca-a39a-572ba01560ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac54e4ac-7768-4b65-adec-0eec91792b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_study1[\"cats_plain\"].map(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006918ac-05a0-4f02-ba08-d160996a0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "303f8084-88ab-4816-9107-d5cad4547a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 59)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.todense().shape  # number samples x vocabulary\n",
    "# vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab42b0b1-e738-4e17-8f36-47d6d5935b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution\n",
      "1    37\n",
      "5    27\n",
      "7    23\n",
      "3    22\n",
      "0    20\n",
      "9    18\n",
      "4    17\n",
      "6    14\n",
      "2    12\n",
      "8    10\n",
      "Name: cats_plain_cluster, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "true_k = 10\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "df_study1[\"cats_plain_cluster\"] = model.predict(X)\n",
    "#df_study1[\"cats_plain_cluster\"]\n",
    "\n",
    "print(\"distribution\")\n",
    "print(df_study1[\"cats_plain_cluster\"].value_counts())\n",
    "\n",
    "#df_study1[df_study1[\"cats_plain_cluster\"] == 1]\n",
    "#df_study1[\"cats_plain_cluster\"].value_counts()\n",
    "\n",
    "#check word freq for cluster==1\n",
    "#df[df.cluster==1].words.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef22f2b-149e-4f15-9863-0603b92cc282",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904487c2-5cf1-48ab-aceb-807ce4276154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "00909e2ca2a2b852db835b984c01db5d56b3a1ad7fadd03544b4638cf5bc5640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
