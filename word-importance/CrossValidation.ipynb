{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import os.path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.metrics import check_scoring, r2_score, accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, check_cv, KFold\n",
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(name)s: %(message)s\")\n",
    "\n",
    "import compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_study_prepared = \"studydata.pickle\"\n",
    "df_study1, df_study2 = compute.load_cached_data(fn_study_prepared)\n",
    "\n",
    "if False:\n",
    "    # fill missing columns\n",
    "    for col in set(df_study2.columns).difference(df_study1.columns):\n",
    "        df_study1.insert(len(df_study1.columns), col, None)\n",
    "\n",
    "    # stack together (columns need to be same)\n",
    "    cols = df_study2.columns.sort_values()\n",
    "    pd.concat([df_study1[cols], df_study2[cols]], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y train/test data\n",
    "\n",
    "# X\n",
    "df_study1_text = df_study1[[compute.COL_TEXT, compute.COL_TEXT_SPACY, compute.COL_TEXT_SPACY_CLEAN]]\n",
    "df_study2_text = df_study2[[compute.COL_TEXT, compute.COL_TEXT_SPACY, compute.COL_TEXT_SPACY_CLEAN]]\n",
    "df_both_text = pd.concat([df_study1_text, df_study2_text], axis=0).reset_index(drop=True)\n",
    "\n",
    "# y\n",
    "df_study1_scores = df_study1[compute.COLS_SCORES]\n",
    "df_study2_scores = df_study2[compute.COLS_SCORES + compute.COLS_SCORES_S2]\n",
    "df_both_scores = pd.concat([df_study1_scores, df_study2_scores[compute.COLS_SCORES]], axis=0).reset_index(drop=True)\n",
    "\n",
    "# X\n",
    "df_study1_liwc = df_study1[compute.COLS_LIWC_REL]\n",
    "df_study2_liwc = df_study2[compute.COLS_LIWC_REL]\n",
    "df_both_liwc = pd.concat([df_study1_liwc, df_study2_liwc], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build standard document-term matrix\n",
    "\n",
    "def build_dtm_df(df_study, binarize=False):\n",
    "    df = df_study[compute.COL_TEXT_SPACY_CLEAN]\n",
    "    doc_term_mat, features = compute.build_feature_matrix(df, norm=\"l2\", use_idf=True)\n",
    "\n",
    "    # binarize (0/1 instead of floats)\n",
    "    if binarize:\n",
    "        doc_term_mat = np.array(np.vectorize(round)(doc_term_mat.todense()))\n",
    "    else:\n",
    "        doc_term_mat = doc_term_mat.toarray()\n",
    "\n",
    "    return pd.DataFrame(doc_term_mat, columns=features)\n",
    "\n",
    "\n",
    "binarize = False\n",
    "# X\n",
    "df_study1_dtm = build_dtm_df(df_study1_text, binarize=binarize)\n",
    "df_study2_dtm = build_dtm_df(df_study2_text, binarize=binarize)\n",
    "df_both_dtm = build_dtm_df(df_both_text, binarize=binarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale LIWC values into [0, 1] range, per column\n",
    "\n",
    "def scale_liwc_df(df_study):\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "\n",
    "    scaler.fit(df_study[compute.COLS_LIWC_REL])\n",
    "    scaler.feature_names_in_\n",
    "    #scaler.scale_, scaler.mean_  # StandardScaler\n",
    "\n",
    "    data = scaler.transform(df_study[compute.COLS_LIWC_REL])\n",
    "    return pd.DataFrame(data, columns=scaler.feature_names_in_)\n",
    "\n",
    "\n",
    "# X\n",
    "df_study1_liwc_scaled = scale_liwc_df(df_study1_liwc)\n",
    "df_study2_liwc_scaled = scale_liwc_df(df_study2_liwc)\n",
    "df_both_liwc_scaled = scale_liwc_df(df_both_liwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse variables into three classes (low/mid/high) based on quantiles\n",
    "\n",
    "def quantize_scores(scores, as_str=False):\n",
    "    # split by quantiles into 3 parts\n",
    "    q33 = np.quantile(scores, 1 / 3)\n",
    "    q66 = np.quantile(scores, 2 / 3)\n",
    "\n",
    "    idx = np.digitize(scores, [q33, q66], right=True)\n",
    "\n",
    "    # validate\n",
    "    # assert all([\n",
    "    #     v <= q33 if x == 0 else v > q33 and v <= q66 if x == 1 else v > q66\n",
    "    #     for v, x in [(scores[i], x) for i, x in enumerate(idx)]\n",
    "    # ])\n",
    "\n",
    "    if not as_str:\n",
    "        return idx\n",
    "\n",
    "    # map to class labels\n",
    "    return np.vectorize({0: \"low\", 1: \"mid\", 2: \"high\"}.get)(idx)\n",
    "\n",
    "\n",
    "def quantize_score_df(df_scores, cols, as_str=False):\n",
    "    all_scores = dict()\n",
    "    for col in cols:\n",
    "        all_scores[col] = quantize_scores(df_scores[col], as_str=as_str)\n",
    "    return pd.DataFrame.from_dict(all_scores)\n",
    "\n",
    "\n",
    "as_str = False\n",
    "# y\n",
    "df_study1_scores_cls = quantize_score_df(df_study1_scores, compute.COLS_SCORES, as_str=as_str)\n",
    "df_study2_scores_cls = quantize_score_df(df_study2_scores, compute.COLS_SCORES + compute.COLS_SCORES_S2, as_str=as_str)\n",
    "df_both_scores_cls = quantize_score_df(df_both_scores, compute.COLS_SCORES, as_str=as_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifiers / Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sels_features = [\"DTM\", \"LIWC\", \"LIWC_S\", \"DTM+LIWC\", \"DTM+LIWC_S\"]\n",
    "sels_study = [\"S1\", \"S2\", \"S1+S2\"]\n",
    "sels_var = [\"power\", \"dominance\", \"prestige\", \"power_f\", \"dominance_f\", \"prestige_f\"]\n",
    "\n",
    "\n",
    "def select_data(sel_features, sel_study):\n",
    "    df_dtm = None\n",
    "    if sel_features == \"DTM\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_dtm\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_dtm\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_dtm\n",
    "    elif sel_features == \"LIWC\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_liwc\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_liwc\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_liwc\n",
    "    elif sel_features == \"LIWC_S\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_liwc_scaled\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_liwc_scaled\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_liwc_scaled\n",
    "    elif sel_features == \"DTM+LIWC\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = pd.concat([df_study1_dtm, df_study1_liwc], axis=1)\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = pd.concat([df_study2_dtm, df_study2_liwc], axis=1)\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = pd.concat([df_both_dtm, df_both_liwc], axis=1)\n",
    "    elif sel_features == \"DTM+LIWC_S\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = pd.concat([df_study1_dtm, df_study1_liwc_scaled], axis=1)\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = pd.concat([df_study2_dtm, df_study2_liwc_scaled], axis=1)\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = pd.concat([df_both_dtm, df_both_liwc_scaled], axis=1)\n",
    "    assert df_dtm is not None\n",
    "    return df_dtm\n",
    "\n",
    "\n",
    "def select_scores(sel_study, sel_var):\n",
    "    df_scores = None\n",
    "    if sel_study == \"S1\":\n",
    "        df_scores = df_study1_scores\n",
    "    elif sel_study == \"S2\":\n",
    "        df_scores = df_study2_scores\n",
    "    elif sel_study == \"S1+S2\":\n",
    "        df_scores = df_both_scores\n",
    "    assert df_scores is not None\n",
    "    col = f\"s:{sel_var}\"\n",
    "    assert col in df_scores.columns\n",
    "    return df_scores[col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(X, y, clf, p_grid, num_trials):\n",
    "    scores = np.zeros(num_trials)\n",
    "    params = []\n",
    "\n",
    "    # Loop for each trial\n",
    "    for i in range(num_trials):\n",
    "        # cross-validation techniques for the inner and outer loops\n",
    "        # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "        inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "        outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "        # Non_nested parameter search and scoring\n",
    "        # clf_gs = GridSearchCV(estimator=clf, param_grid=p_grid, cv=outer_cv)\n",
    "        # clf_gs.fit(X, y)\n",
    "        # clf.cv_results_\n",
    "\n",
    "        # Nested CV with parameter optimization\n",
    "        clf_gs = GridSearchCV(estimator=clf, param_grid=p_grid, cv=inner_cv, verbose=0)\n",
    "        #score = cross_val_score(clf_gs, X=X, y=y, cv=outer_cv)\n",
    "\n",
    "        scorer = check_scoring(clf_gs, scoring=None)\n",
    "        cv_results = cross_validate(\n",
    "            estimator=clf_gs,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            scoring={\"score\": scorer},\n",
    "            cv=outer_cv,\n",
    "            return_estimator=True\n",
    "        )\n",
    "        score = cv_results[\"test_score\"]\n",
    "        best_params = cv_results[\"estimator\"][score.argmax()].best_params_\n",
    "\n",
    "        scores[i] = score.mean()\n",
    "        params.append(best_params)\n",
    "\n",
    "    return scores, params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_study = \"S1+S2\"         # one of: S1, S2, S1+S2\n",
    "sel_features = \"DTM+LIWC\"   # one of: DTM, LIWC, LIWC_S, DTM+LIWC, DTM+LIWC_S\n",
    "sel_var = \"power\"           # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "\n",
    "X = select_data(sel_features, sel_study).values\n",
    "y = select_scores(sel_study, sel_var)\n",
    "\n",
    "# convert continuous y variable to class variable\n",
    "#y_cls = y.round()\n",
    "#y_cls = y.astype(int)\n",
    "y_cls = np.vectorize(round)(y)\n",
    "y_lmh = quantize_scores(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'gamma': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_23a45_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_23a45\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_23a45_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_23a45_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_23a45_row0_col0\" class=\"data row0 col0\" >0.345000</td>\n",
       "      <td id=\"T_23a45_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_23a45_row1_col0\" class=\"data row1 col0\" >0.345000</td>\n",
       "      <td id=\"T_23a45_row1_col1\" class=\"data row1 col1\" >{'C': 10, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_23a45_row2_col0\" class=\"data row2 col0\" >0.327500</td>\n",
       "      <td id=\"T_23a45_row2_col1\" class=\"data row2 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_23a45_row3_col0\" class=\"data row3 col0\" >0.335000</td>\n",
       "      <td id=\"T_23a45_row3_col1\" class=\"data row3 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_23a45_row4_col0\" class=\"data row4 col0\" >0.357500</td>\n",
       "      <td id=\"T_23a45_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbc9c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"rbf\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "scores, params = run_trials(X, y_cls, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame.from_dict(dict(zip(scores, params)))\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'gamma': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ec9c9_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ec9c9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ec9c9_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_ec9c9_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ec9c9_row0_col0\" class=\"data row0 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ec9c9_row1_col0\" class=\"data row1 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row1_col1\" class=\"data row1 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ec9c9_row2_col0\" class=\"data row2 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row2_col1\" class=\"data row2 col1\" >{'C': 10, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ec9c9_row3_col0\" class=\"data row3 col0\" >0.327500</td>\n",
       "      <td id=\"T_ec9c9_row3_col1\" class=\"data row3 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ec9c9_row4_col0\" class=\"data row4 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'gamma': 0.1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad10c070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"rbf\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 0.01, 'max_iter': 10}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_560fd_row1_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_560fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_560fd_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_560fd_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_560fd_row0_col0\" class=\"data row0 col0\" >0.320000</td>\n",
       "      <td id=\"T_560fd_row0_col1\" class=\"data row0 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_560fd_row1_col0\" class=\"data row1 col0\" >0.342500</td>\n",
       "      <td id=\"T_560fd_row1_col1\" class=\"data row1 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_560fd_row2_col0\" class=\"data row2 col0\" >0.275000</td>\n",
       "      <td id=\"T_560fd_row2_col1\" class=\"data row2 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_560fd_row3_col0\" class=\"data row3 col0\" >0.282500</td>\n",
       "      <td id=\"T_560fd_row3_col1\" class=\"data row3 col1\" >{'C': 0.01, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_560fd_row4_col0\" class=\"data row4 col0\" >0.297500</td>\n",
       "      <td id=\"T_560fd_row4_col1\" class=\"data row4 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbd2f40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X, y_cls, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'max_iter': 10}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9f816_row2_col0, #T_9f816_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9f816\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9f816_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_9f816_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9f816_row0_col0\" class=\"data row0 col0\" >0.370000</td>\n",
       "      <td id=\"T_9f816_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9f816_row1_col0\" class=\"data row1 col0\" >0.355000</td>\n",
       "      <td id=\"T_9f816_row1_col1\" class=\"data row1 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9f816_row2_col0\" class=\"data row2 col0\" >0.415000</td>\n",
       "      <td id=\"T_9f816_row2_col1\" class=\"data row2 col1\" >{'C': 1, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9f816_row3_col0\" class=\"data row3 col0\" >0.397500</td>\n",
       "      <td id=\"T_9f816_row3_col1\" class=\"data row3 col1\" >{'C': 0.1, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9f816_row4_col0\" class=\"data row4 col0\" >0.415000</td>\n",
       "      <td id=\"T_9f816_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad037970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'alpha': 1, 'max_iter': 5000, 'selection': 'random'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_afa3e_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_afa3e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_afa3e_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_afa3e_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_afa3e_row0_col0\" class=\"data row0 col0\" >-0.005240</td>\n",
       "      <td id=\"T_afa3e_row0_col1\" class=\"data row0 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_afa3e_row1_col0\" class=\"data row1 col0\" >-0.011109</td>\n",
       "      <td id=\"T_afa3e_row1_col1\" class=\"data row1 col1\" >{'alpha': 1, 'max_iter': 1000, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_afa3e_row2_col0\" class=\"data row2 col0\" >-0.011825</td>\n",
       "      <td id=\"T_afa3e_row2_col1\" class=\"data row2 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_afa3e_row3_col0\" class=\"data row3 col0\" >-0.023269</td>\n",
       "      <td id=\"T_afa3e_row3_col1\" class=\"data row3 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_afa3e_row4_col0\" class=\"data row4 col0\" >0.009863</td>\n",
       "      <td id=\"T_afa3e_row4_col1\" class=\"data row4 col1\" >{'alpha': 1, 'max_iter': 5000, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad10c0a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.Lasso()\n",
    "p_grid = {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"random\", \"cyclic\"]}\n",
    "\n",
    "scores, params = run_trials(X, y, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'alpha': 10, 'max_iter': 250, 'selection': 'random'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7100b_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7100b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7100b_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_7100b_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7100b_row0_col0\" class=\"data row0 col0\" >-0.018498</td>\n",
       "      <td id=\"T_7100b_row0_col1\" class=\"data row0 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7100b_row1_col0\" class=\"data row1 col0\" >-0.027791</td>\n",
       "      <td id=\"T_7100b_row1_col1\" class=\"data row1 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7100b_row2_col0\" class=\"data row2 col0\" >-0.016603</td>\n",
       "      <td id=\"T_7100b_row2_col1\" class=\"data row2 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7100b_row3_col0\" class=\"data row3 col0\" >-0.007605</td>\n",
       "      <td id=\"T_7100b_row3_col1\" class=\"data row3 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7100b_row4_col0\" class=\"data row4 col0\" >-0.001174</td>\n",
       "      <td id=\"T_7100b_row4_col1\" class=\"data row4 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbc63d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this might not be best, we do some linear model but with classes?\n",
    "\n",
    "clf = sklearn.linear_model.Lasso()\n",
    "p_grid = {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"random\", \"cyclic\"]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame.from_dict(dict(zip(scores, params)))\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test matrix\n",
    "\n",
    "over _Study_, _Variables_, _Features_ and _Estimators_ with Hyper-Parameter Search  \n",
    "to find best combination of data, features and models for reliable predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ----------------------------------------\n",
      "# Running for Study: S1 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.026192\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.019900\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ -0.001337 ±0.029382\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.290000 ±0.010296\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.019539 ±0.029860\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.380000 ±0.012083\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.295000 ±0.030757\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.295000 ±0.015297\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.019628 ±0.029884\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.010677\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.300000 ±0.024779\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.305000 ±0.024207\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.260000 ±0.009274\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.011175 ±0.015611\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.280000 ±0.022000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014596\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.315000 ±0.033226\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.270000 ±0.020881\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.275000 ±0.020833\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014599\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.300000 ±0.028107\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.275000 ±0.021307\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.004000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.485000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ -0.003494 ±0.015369\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.033971\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010938 ±0.019912\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.025612\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.016912\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008639 ±0.024615\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.037815\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.010957 ±0.019882\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.032156\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.017029\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008626 ±0.024608\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.027857\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.395000 ±0.033257\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'random'} @ -0.007793 ±0.014259\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.028000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.037940 ±0.033412\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.024819\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.290000 ±0.011225\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002120 ±0.016022\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.026192\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.037729 ±0.033352\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.028566\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.280000 ±0.011402\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ -0.002121 ±0.016009\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.420000 ±0.030757\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.410000 ±0.008124\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006658 ±0.013335\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.019647\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.024495\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002467 ±0.016662\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.021213\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.028705\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.355000 ±0.026000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ -0.002461 ±0.016663\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.570000 ±0.004899\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.560000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003361 ±0.020323\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.009274\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099824 ±0.017418\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.017493\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.121973 ±0.011753\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.007348\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099835 ±0.017425\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.015937\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.122015 ±0.011761\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S2 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022045\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.012410\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042992\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.018815\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042253\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.018601\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023875\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003203 ±0.035641\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.016852\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042255\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.016912\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023152\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003216 ±0.035640\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.365000 ±0.015166\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.375000 ±0.019339\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018157 ±0.013030\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.415000 ±0.018276\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.020833\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.370000 ±0.022226\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039193 ±0.010758\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.420000 ±0.016000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018330\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.380000 ±0.022891\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039192 ±0.010754\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.510000 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.019554 ±0.061176\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023958\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.055881 ±0.076484\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.014353\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.034986\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.005339 ±0.066582\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023367\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.055912 ±0.076491\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.020494\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.029428\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.005334 ±0.066585\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.011662\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.470000 ±0.004000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005932 ±0.019277\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.022450\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.006211 ±0.021080\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.019900\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.440000 ±0.027092\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007654 ±0.017074\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.025020\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.006208 ±0.021087\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.018276\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.435000 ±0.020591\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007656 ±0.017062\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.510000 ±0.051536\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.485000 ±0.042544\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001310 ±0.021467\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.445000 ±0.022494\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.005581 ±0.029849\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.530000 ±0.055534\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.445000 ±0.020248\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020825 ±0.025905\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.021213\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.005579 ±0.029832\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.515000 ±0.031401\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.455000 ±0.019494\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020819 ±0.025921\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.655000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.008781 ±0.009376\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.015033\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010837 ±0.030403\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.520000 ±0.013784\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.000607 ±0.014225\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.014000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010839 ±0.030405\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.530000 ±0.016432\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.000605 ±0.014232\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S1+S2 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022215\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.367500 ±0.019144\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003178 ±0.006021\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.332500 ±0.021829\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.009811 ±0.010786\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.014832\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.370000 ±0.029326\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.026347 ±0.013033\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.342500 ±0.024829\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.009801 ±0.010763\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.342500 ±0.011979\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.365000 ±0.026907\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.026359 ±0.013023\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.014883\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.347500 ±0.010173\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005442 ±0.009491\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.332500 ±0.009798\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008402\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.360000 ±0.015166\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.340000 ±0.019196\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.013657\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.007829\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.357500 ±0.014457\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.330000 ±0.018615\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.005000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.497500 ±0.002000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.000597 ±0.009936\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.437500 ±0.015297\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.035048 ±0.007926\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.011979\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016275 ±0.010817\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.440000 ±0.015572\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.035040 ±0.007941\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.010886\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016237 ±0.010794\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.016000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.427500 ±0.002000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001951 ±0.012982\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.011554\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043887 ±0.013905\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.010368\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.385000 ±0.020248\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059043 ±0.012958\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043922 ±0.013916\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.011247\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.390000 ±0.021943\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059042 ±0.012957\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.447500 ±0.016386\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.432500 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003755 ±0.010660\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.020000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.010381 ±0.021757\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.445000 ±0.015297\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.390000 ±0.019975\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024462 ±0.010246\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.405000 ±0.020000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010431 ±0.021781\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.442500 ±0.013928\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.387500 ±0.013730\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024464 ±0.010393\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.610000 ±0.004472\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.607500 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001399 ±0.011015\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.562500 ±0.008216\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.144116 ±0.016951\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003873\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.577500 ±0.014748\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ 0.092797 ±0.014769\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.572500 ±0.009925\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.144117 ±0.016956\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003742\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.570000 ±0.012981\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ 0.092799 ±0.014768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimators and Parameter-Spaces\n",
    "clfs = [\n",
    "    (sklearn.svm.SVC(kernel=\"rbf\"),\n",
    "     {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}),\n",
    "    (sklearn.linear_model.LogisticRegression(solver=\"liblinear\"),\n",
    "     {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}),\n",
    "]\n",
    "regrs = [\n",
    "    (sklearn.linear_model.Lasso(),\n",
    "     {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"cyclic\", \"random\"]}),\n",
    "]\n",
    "num_runs = 5\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    results.setdefault(sel_study, dict())\n",
    "    print(\"#\", \"-\" * 40)\n",
    "    print(f\"# Running for Study: {sel_study} ...\")\n",
    "    for sel_var in sels_var:\n",
    "        results[sel_study].setdefault(sel_var, dict())\n",
    "        print(f\"## Running for Variable: *** {sel_var} *** ...\")\n",
    "        for sel_features in sels_features:\n",
    "            results[sel_study][sel_var].setdefault(sel_features, dict())\n",
    "            print(f\"### Running on Feature-Set: {sel_features} ...\")\n",
    "\n",
    "            # Select data\n",
    "            X = select_data(sel_features, sel_study).values\n",
    "            y = select_scores(sel_study, sel_var)\n",
    "            y_cls = np.vectorize(round)(y)\n",
    "\n",
    "            # Run classifiers\n",
    "            for clf, p_grid in clfs:\n",
    "                print(f\"#### Clf: {clf}\")\n",
    "                scores, params = run_trials(X, y_cls, clf, p_grid, num_runs)\n",
    "                print(f\"     best params: {params[scores.argmax()]} @ {scores.max():f} ±{scores.std():f}\")\n",
    "                cur_results = pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"])\n",
    "                results[sel_study][sel_var][sel_features][clf.__class__.__name__.rsplit(\".\", 1)[-1]] = cur_results\n",
    "\n",
    "            # Run regressors\n",
    "            for reg, p_grid in regrs:\n",
    "                print(f\"#### Regr: {reg}\")\n",
    "                scores, params = run_trials(X, y, reg, p_grid, num_runs)\n",
    "                print(f\"     best params: {params[scores.argmax()]} @ {scores.max():f} ±{scores.std():f}\")\n",
    "                cur_results = pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"])\n",
    "                results[sel_study][sel_var][sel_features][reg.__class__.__name__.rsplit(\".\", 1)[-1]] = cur_results\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "fn_results = \"results.pickle\"\n",
    "if not os.path.exists(fn_results):\n",
    "    with open(fn_results, \"wb\") as fp:\n",
    "        pickle.dump(results, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(fn_results, \"rb\") as fp:\n",
    "    results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ----------------------------------------\n",
      "# Running for Study: S1 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.026192\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.019900\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ -0.001337 ±0.029382\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.290000 ±0.010296\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.019539 ±0.029860\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.380000 ±0.012083\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.295000 ±0.030757\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.295000 ±0.015297\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.019628 ±0.029884\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.010677\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.300000 ±0.024779\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.305000 ±0.024207\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.260000 ±0.009274\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.011175 ±0.015611\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.280000 ±0.022000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014596\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.315000 ±0.033226\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.270000 ±0.020881\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.275000 ±0.020833\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014599\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.300000 ±0.028107\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.275000 ±0.021307\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.004000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.485000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ -0.003494 ±0.015369\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.033971\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010938 ±0.019912\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.025612\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.016912\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008639 ±0.024615\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.037815\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.010957 ±0.019882\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.032156\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.017029\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008626 ±0.024608\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.027857\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.395000 ±0.033257\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'random'} @ -0.007793 ±0.014259\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.028000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.037940 ±0.033412\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.024819\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.290000 ±0.011225\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002120 ±0.016022\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.026192\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.037729 ±0.033352\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.028566\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.280000 ±0.011402\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ -0.002121 ±0.016009\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.420000 ±0.030757\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.410000 ±0.008124\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006658 ±0.013335\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.019647\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.024495\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002467 ±0.016662\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.021213\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.028705\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.355000 ±0.026000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ -0.002461 ±0.016663\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.570000 ±0.004899\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.560000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003361 ±0.020323\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.009274\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099824 ±0.017418\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.017493\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.121973 ±0.011753\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.007348\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099835 ±0.017425\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.015937\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.122015 ±0.011761\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S2 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022045\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.012410\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042992\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.018815\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042253\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.018601\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023875\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003203 ±0.035641\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.016852\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042255\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.016912\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023152\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003216 ±0.035640\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.365000 ±0.015166\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.375000 ±0.019339\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018157 ±0.013030\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.415000 ±0.018276\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.020833\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.370000 ±0.022226\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039193 ±0.010758\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.420000 ±0.016000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018330\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.380000 ±0.022891\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039192 ±0.010754\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.510000 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.019554 ±0.061176\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023958\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.055881 ±0.076484\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.014353\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.034986\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.005339 ±0.066582\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023367\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.055912 ±0.076491\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.020494\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.029428\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.005334 ±0.066585\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.011662\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.470000 ±0.004000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005932 ±0.019277\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.022450\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.006211 ±0.021080\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.019900\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.440000 ±0.027092\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007654 ±0.017074\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.025020\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.006208 ±0.021087\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.018276\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.435000 ±0.020591\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007656 ±0.017062\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.510000 ±0.051536\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.485000 ±0.042544\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001310 ±0.021467\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.445000 ±0.022494\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.005581 ±0.029849\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.530000 ±0.055534\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.445000 ±0.020248\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020825 ±0.025905\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.021213\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.005579 ±0.029832\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.515000 ±0.031401\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.455000 ±0.019494\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020819 ±0.025921\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.655000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.008781 ±0.009376\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.015033\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010837 ±0.030403\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.520000 ±0.013784\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.000607 ±0.014225\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.014000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010839 ±0.030405\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.530000 ±0.016432\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.000605 ±0.014232\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S1+S2 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022215\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.367500 ±0.019144\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003178 ±0.006021\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.332500 ±0.021829\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.009811 ±0.010786\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.014832\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.370000 ±0.029326\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.026347 ±0.013033\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.342500 ±0.024829\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.009801 ±0.010763\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.342500 ±0.011979\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.365000 ±0.026907\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.026359 ±0.013023\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.014883\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.347500 ±0.010173\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005442 ±0.009491\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.332500 ±0.009798\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008402\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.360000 ±0.015166\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.340000 ±0.019196\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.013657\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.007829\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.357500 ±0.014457\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.330000 ±0.018615\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.005000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.497500 ±0.002000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.000597 ±0.009936\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.437500 ±0.015297\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.035048 ±0.007926\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.011979\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016275 ±0.010817\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.440000 ±0.015572\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.035040 ±0.007941\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.010886\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016237 ±0.010794\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.016000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.427500 ±0.002000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001951 ±0.012982\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.011554\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043887 ±0.013905\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.010368\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.385000 ±0.020248\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059043 ±0.012958\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043922 ±0.013916\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.011247\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.390000 ±0.021943\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059042 ±0.012957\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.447500 ±0.016386\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.432500 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003755 ±0.010660\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.020000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.010381 ±0.021757\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.445000 ±0.015297\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.390000 ±0.019975\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024462 ±0.010246\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.405000 ±0.020000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010431 ±0.021781\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.442500 ±0.013928\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.387500 ±0.013730\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024464 ±0.010393\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.610000 ±0.004472\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.607500 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001399 ±0.011015\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.562500 ±0.008216\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.144116 ±0.016951\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003873\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.577500 ±0.014748\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ 0.092797 ±0.014769\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.572500 ±0.009925\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.144117 ±0.016956\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003742\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.570000 ±0.012981\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ 0.092799 ±0.014768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out previous results (simulate matrix run, just print selection and scores/params)\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    print(\"#\", \"-\" * 40)\n",
    "    print(f\"# Running for Study: {sel_study} ...\")\n",
    "    for sel_var in sels_var:\n",
    "        print(f\"## Running for Variable: *** {sel_var} *** ...\")\n",
    "        for sel_features in sels_features:\n",
    "            print(f\"### Running on Feature-Set: {sel_features} ...\")\n",
    "\n",
    "            for est, df in results[sel_study][sel_var][sel_features].items():\n",
    "                print(f\"#### Estimator: {est}\")\n",
    "                scores = df[\"scores\"].values\n",
    "                best_params = df[\"params\"][scores.argmax()]\n",
    "                print(f\"     best params: {best_params} @ {scores.max():f} ±{scores.std():f}\")\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in Data from Study: S1\n",
      "  Best results for >>>power<<< on data: 'LIWC' with 0.374000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.375000 ±0.002000\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'DTM' with 0.262000\n",
      "  -> Clf: SVC with params: {'C': 10, 'gamma': 0.1}; 0.305000 ±0.024207\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'DTM' with 0.485000\n",
      "  -> Clf: LogisticRegression with params: {'C': 0.1, 'max_iter': 10}; 0.485000 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'DTM' with 0.363000\n",
      "  -> Clf: LogisticRegression with params: {'C': 1, 'max_iter': 10}; 0.395000 ±0.033257\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'LIWC' with 0.400000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.400000 ±0.000000\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'LIWC_S' with 0.572000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.580000 ±0.009274\n",
      "\n",
      "Results in Data from Study: S2\n",
      "  Best results for >>>power<<< on data: 'DTM' with 0.361000\n",
      "  -> Clf: LogisticRegression with params: {'C': 1, 'max_iter': 10}; 0.375000 ±0.012410\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'DTM+LIWC' with 0.392000\n",
      "  -> Clf: LogisticRegression with params: {'C': 0.1, 'max_iter': 10}; 0.420000 ±0.016000\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'DTM' with 0.510000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.510000 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'LIWC' with 0.470000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.470000 ±0.000000\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'DTM+LIWC_S' with 0.457000\n",
      "  -> Clf: SVC with params: {'C': 10, 'gamma': 0.01}; 0.515000 ±0.031401\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'DTM' with 0.655000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.655000 ±0.000000\n",
      "\n",
      "Results in Data from Study: S1+S2\n",
      "  Best results for >>>power<<< on data: 'LIWC' with 0.342000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.357500 ±0.010173\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'LIWC' with 0.340000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.355000 ±0.016508\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'LIWC' with 0.497500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.497500 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'LIWC' with 0.427500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.427500 ±0.000000\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'LIWC' with 0.432500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.432500 ±0.000000\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'LIWC_S' with 0.612500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.615000 ±0.003873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out best results for each study-variable\n",
    "# (aggregating over feature and estimator)\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    print(f\"Results in Data from Study: {sel_study}\")\n",
    "\n",
    "    for sel_var in sels_var:\n",
    "        best = None\n",
    "        for sel_features in sels_features:\n",
    "            for clf, df_results in results[sel_study][sel_var][sel_features].items():\n",
    "                scores = df_results[\"scores\"].values\n",
    "                score = scores.mean()\n",
    "                if best is None or score > best[0]:\n",
    "                    best = (score, df_results, clf, sel_features)\n",
    "        print(f\"  Best results for >>>{sel_var}<<< on data: '{best[3]}' with {best[0]:f}\")\n",
    "        scores = best[1][\"scores\"].values\n",
    "        best_params = best[1][\"params\"][scores.argmax()]\n",
    "        print(f\"  -> Clf: {best[2]} with params: {best_params}; {scores.max():f} ±{scores.std():f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize results to excel sheet\n",
    "\n",
    "index = []\n",
    "data = []\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    for sel_var in sels_var:\n",
    "        for sel_features in sels_features:\n",
    "\n",
    "            for est, df in results[sel_study][sel_var][sel_features].items():\n",
    "                index.append([sel_study, sel_var, sel_features, est])\n",
    "                scores = df[\"scores\"].values\n",
    "                best_params = df[\"params\"][scores.argmax()]\n",
    "                data.append(scores.tolist() + [scores.max(), scores.std(), best_params])\n",
    "\n",
    "df_results = pd.DataFrame(data,\n",
    "             columns=[f\"Run {i}\" for i in range(num_runs)] + [\"mean\", \"avg\", \"best estimator params\"],\n",
    "             index=pd.MultiIndex.from_frame(pd.DataFrame(index, columns=[\"Study\", \"Variable\", \"Features\", \"Estimator\"]))).T\n",
    "\n",
    "fn_results_xls = \"results.xlsx\"\n",
    "df_results.to_excel(fn_results_xls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train estimator with 'best' parameters\n",
    "\n",
    "- Run hyper-parameter search with cross-validation &rarr; find best params\n",
    "- Train on one study, then evaluate on other study data - best cross-validation (since unseen)\n",
    "- Compute scores for Accuracy, R²\n",
    "- Extract coefficients of best model (&rarr; what features have highest influence on prediction, based on training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = \"LIWC\"       # one of: DTM, LIWC, LIWC_S, B (DTM, LIWC), B_S (DTM, LIWC_S)\n",
    "sel_var = \"power\"    # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "\n",
    "sel_features_train = \"S2\"\n",
    "sel_features_test = \"S1\"\n",
    "\n",
    "X = select_data(sel_data, sel_features_train)\n",
    "X_train = X.values\n",
    "X_labels = X.columns.values\n",
    "y_train = select_scores(sel_features_train, sel_var)\n",
    "y_train_cls = quantize_scores(y_train)\n",
    "\n",
    "X_test = select_data(sel_data, sel_features_test).values\n",
    "y_test = select_scores(sel_features_test, sel_var)\n",
    "y_test_cls = quantize_scores(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'C': 1, 'max_iter': 100} with 0.385000 (0.348000 ±0.022271)\n"
     ]
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"linear\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X_train, y_train_cls, clf, p_grid, 5)\n",
    "best_params = params[scores.argmax()]\n",
    "\n",
    "print(f\"best params: {best_params} with {scores.max():f} ({scores.mean():f} ±{scores.std():f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cross-validation score: 0.410000\n",
      "\n",
      "R² Score train: 0.5521235521235521, test: -0.4437462597247157\n",
      "Accuracy Score train: 0.815, test: 0.41\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(**best_params)\n",
    "clf.fit(X_train, y_train_cls)\n",
    "score = clf.score(X_test, y_test_cls)\n",
    "print(f\"train cross-validation score: {score:f}\")\n",
    "print()\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# \"overfitting\" on train dataset should often only return 1.0 (max) as score\n",
    "print(f\"R² Score train: {r2_score(y_train_cls, y_train_pred)}, test: {r2_score(y_test_cls, y_test_pred)}\")\n",
    "print(f\"Accuracy Score train: {accuracy_score(y_train_cls, y_train_pred)}, test: {accuracy_score(y_test_cls, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00*'risk' + 0.89*'reward' + 0.87*'affiliation' + 0.76*'body' + 0.75*'see' + 0.69*'we' + 0.65*'sad' + 0.59*'number' + 0.55*'QMark' + 0.54*'posemo' + 0.53*'bio' + -0.50*'Quote' + -0.62*'time' + -0.68*'health' + -0.69*'anx' + -0.72*'anger' + -0.91*'ingest' + -0.95*'other' + -0.97*'feel' + -1.00*'assent'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract coefficient, normalize them and filter by importance, then plot\n",
    "\n",
    "coefs = clf.coef_.copy()  # only kernel=\"linear\" in SVC; or LogReg\n",
    "\n",
    "coefs = coefs * np.linspace(-1, 1, len(clf.classes_))[:,np.newaxis]\n",
    "coefs = coefs.sum(axis=0)\n",
    "coefs = compute.normalize_coefs(coefs)\n",
    "\n",
    "values, labels = compute.coef_filter(coefs, X_labels)\n",
    "print(compute.coef_to_human(values, labels))\n",
    "\n",
    "df_coefs = pd.DataFrame(values, index=labels)\n",
    "df_coefs.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.260718798707047"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "sel_study = \"S1\"      # one of: S1, S2, S1+S2\n",
    "sel_var = \"power\"     # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "                      #         optionally also: workplace_power, workplace_power_f  (only S2)\n",
    "\n",
    "y = select_scores(sel_study, sel_var)\n",
    "\n",
    "# convert continuous y variable to class variable\n",
    "#y_cls = y.round()\n",
    "#y_cls = y.astype(int)\n",
    "y_cls = np.vectorize(round)(y)\n",
    "y_lmh = quantize_scores(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression\n",
    "\n",
    "# dummy, baseline predictions\n",
    "y_pred_mean = np.full_like(y, fill_value=np.mean(y))\n",
    "y_pred_median = np.full_like(y, fill_value=np.median(y))\n",
    "y_pred_oov = np.full_like(y, -1)  # ?\n",
    "y_pred_rnd = np.random.rand(*y.shape) * 7\n",
    "\n",
    "# scores\n",
    "s_r2_true = r2_score(y, y)\n",
    "s_r2_mean = r2_score(y, y_pred_mean)\n",
    "s_r2_median = r2_score(y, y_pred_median)\n",
    "s_r2_oov = r2_score(y, y_pred_oov)\n",
    "s_r2_rnd = r2_score(y, y_pred_rnd)\n",
    "\n",
    "s_mse_true = mean_squared_error(y, y)\n",
    "s_mse_mean = mean_squared_error(y, y_pred_mean)\n",
    "s_mse_median = mean_squared_error(y, y_pred_median)\n",
    "s_mse_oov = mean_squared_error(y, y_pred_oov)\n",
    "s_mse_rnd = mean_squared_error(y, y_pred_rnd)\n",
    "\n",
    "# overview table\n",
    "s_reg_df = pd.DataFrame(\n",
    "    [s_r2_true, s_r2_mean, s_r2_median, s_r2_oov, s_r2_rnd,\n",
    "     s_mse_true, s_mse_mean, s_mse_median, s_mse_oov, s_mse_rnd],\n",
    "    columns=[\"score\"],\n",
    "    index=pd.MultiIndex.from_product([[\"r2\", \"mse\"], [\"y_true\", \"mean\", \"median\", \"oov\", \"random\"]], names=[\"Measure\", \"y-values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification\n",
    "\n",
    "# dummy, baseline predictions\n",
    "y_17_pred_mean = np.full_like(y_cls, fill_value=np.mean(y_cls))\n",
    "y_17_pred_median = np.full_like(y_cls, fill_value=np.median(y_cls))\n",
    "y_17_pred_oov = np.full_like(y_cls, -1)  # ?\n",
    "y_17_pred_rnd = np.random.randint(7, size=y_cls.shape)  # or quantize_scores(np.random.rand(*y.shape))\n",
    "\n",
    "y_lmh_pred_mean = np.full_like(y_lmh, fill_value=np.mean(y_lmh))\n",
    "y_lmh_pred_median = np.full_like(y_lmh, fill_value=np.median(y_lmh))\n",
    "y_lmh_pred_oov = np.full_like(y_lmh, -1)  # ?\n",
    "y_lmh_pred_rnd = np.random.randint(3, size=y_lmh.shape)\n",
    "\n",
    "# scores\n",
    "s_17_acc_true = accuracy_score(y_cls, y_cls)\n",
    "s_17_acc_mean = accuracy_score(y_cls, y_17_pred_mean)\n",
    "s_17_acc_median = accuracy_score(y_cls, y_17_pred_median)\n",
    "s_17_acc_oov = accuracy_score(y_cls, y_17_pred_oov)\n",
    "s_17_acc_rnd = accuracy_score(y_cls, y_17_pred_rnd)\n",
    "\n",
    "s_lmh_acc_true = accuracy_score(y_lmh, y_lmh)\n",
    "s_lmh_acc_mean = accuracy_score(y_lmh, y_lmh_pred_mean)\n",
    "s_lmh_acc_median = accuracy_score(y_lmh, y_lmh_pred_median)\n",
    "s_lmh_acc_oov = accuracy_score(y_lmh, y_lmh_pred_oov)\n",
    "s_lmh_acc_rnd = accuracy_score(y_lmh, y_lmh_pred_rnd)\n",
    "\n",
    "# overview table\n",
    "s_clf_df = pd.DataFrame(\n",
    "    [s_17_acc_true, s_17_acc_mean, s_17_acc_median, s_17_acc_oov, s_17_acc_rnd,\n",
    "     s_lmh_acc_true, s_lmh_acc_mean, s_lmh_acc_median, s_lmh_acc_oov, s_lmh_acc_rnd],\n",
    "    columns=[\"accuracy\"],\n",
    "    index=pd.MultiIndex.from_product(\n",
    "        [[\"1..7\", \"low/mid/high\"], [\"y_true\", \"mean\", \"median\", \"oov\", \"rnd\"]],\n",
    "        names=[\"Classes\", \"y-values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Measure</th>\n",
       "      <th colspan=\"5\" halign=\"left\">r2</th>\n",
       "      <th colspan=\"5\" halign=\"left\">mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y-values</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>random</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-37.273581</td>\n",
       "      <td>-6.117469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.969716</td>\n",
       "      <td>0.982473</td>\n",
       "      <td>37.114504</td>\n",
       "      <td>6.901923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines classification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Classes</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1..7</th>\n",
       "      <th colspan=\"5\" halign=\"left\">low/mid/high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y-values</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>rnd</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>rnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Baselines regression:\")\n",
    "display_html(s_reg_df.T)\n",
    "print(\"Baselines classification:\")\n",
    "display_html(s_clf_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
