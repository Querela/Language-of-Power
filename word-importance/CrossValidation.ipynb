{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "import os.path\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.metrics import check_scoring, r2_score, accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, check_cv, KFold\n",
    "\n",
    "from IPython.display import display_html\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(levelname)s] %(name)s: %(message)s\")\n",
    "\n",
    "import compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_study_prepared = \"studydata.pickle\"\n",
    "df_study1, df_study2 = compute.load_cached_data(fn_study_prepared)\n",
    "\n",
    "if False:\n",
    "    # fill missing columns\n",
    "    for col in set(df_study2.columns).difference(df_study1.columns):\n",
    "        df_study1.insert(len(df_study1.columns), col, None)\n",
    "\n",
    "    # stack together (columns need to be same)\n",
    "    cols = df_study2.columns.sort_values()\n",
    "    pd.concat([df_study1[cols], df_study2[cols]], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y train/test data\n",
    "\n",
    "# X\n",
    "df_study1_text = df_study1[[compute.COL_TEXT, compute.COL_TEXT_SPACY, compute.COL_TEXT_SPACY_CLEAN]]\n",
    "df_study2_text = df_study2[[compute.COL_TEXT, compute.COL_TEXT_SPACY, compute.COL_TEXT_SPACY_CLEAN]]\n",
    "df_both_text = pd.concat([df_study1_text, df_study2_text], axis=0).reset_index(drop=True)\n",
    "\n",
    "# y\n",
    "df_study1_scores = df_study1[compute.COLS_SCORES]\n",
    "df_study2_scores = df_study2[compute.COLS_SCORES + compute.COLS_SCORES_S2]\n",
    "df_both_scores = pd.concat([df_study1_scores, df_study2_scores[compute.COLS_SCORES]], axis=0).reset_index(drop=True)\n",
    "\n",
    "# X\n",
    "df_study1_liwc = df_study1[compute.COLS_LIWC_REL]\n",
    "df_study2_liwc = df_study2[compute.COLS_LIWC_REL]\n",
    "df_both_liwc = pd.concat([df_study1_liwc, df_study2_liwc], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build standard document-term matrix\n",
    "\n",
    "def build_dtm_df(df_study, binarize=False):\n",
    "    df = df_study[compute.COL_TEXT_SPACY_CLEAN]\n",
    "    doc_term_mat, features = compute.build_feature_matrix(df, norm=\"l2\", use_idf=True)\n",
    "\n",
    "    # binarize (0/1 instead of floats)\n",
    "    if binarize:\n",
    "        doc_term_mat = np.array(np.vectorize(round)(doc_term_mat.todense()))\n",
    "    else:\n",
    "        doc_term_mat = doc_term_mat.toarray()\n",
    "\n",
    "    return pd.DataFrame(doc_term_mat, columns=features)\n",
    "\n",
    "\n",
    "binarize = False\n",
    "# X\n",
    "df_study1_dtm = build_dtm_df(df_study1_text, binarize=binarize)\n",
    "df_study2_dtm = build_dtm_df(df_study2_text, binarize=binarize)\n",
    "df_both_dtm = build_dtm_df(df_both_text, binarize=binarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale LIWC values into [0, 1] range, per column\n",
    "\n",
    "def scale_liwc_df(df_study):\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = MinMaxScaler()\n",
    "\n",
    "    scaler.fit(df_study[compute.COLS_LIWC_REL])\n",
    "    scaler.feature_names_in_\n",
    "    #scaler.scale_, scaler.mean_  # StandardScaler\n",
    "\n",
    "    data = scaler.transform(df_study[compute.COLS_LIWC_REL])\n",
    "    return pd.DataFrame(data, columns=scaler.feature_names_in_)\n",
    "\n",
    "\n",
    "# X\n",
    "df_study1_liwc_scaled = scale_liwc_df(df_study1_liwc)\n",
    "df_study2_liwc_scaled = scale_liwc_df(df_study2_liwc)\n",
    "df_both_liwc_scaled = scale_liwc_df(df_both_liwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapse variables into three classes (low/mid/high) based on quantiles\n",
    "\n",
    "def quantize_scores(scores, as_str=False):\n",
    "    # split by quantiles into 3 parts\n",
    "    q33 = np.quantile(scores, 1 / 3)\n",
    "    q66 = np.quantile(scores, 2 / 3)\n",
    "\n",
    "    idx = np.digitize(scores, [q33, q66], right=True)\n",
    "\n",
    "    # validate\n",
    "    # assert all([\n",
    "    #     v <= q33 if x == 0 else v > q33 and v <= q66 if x == 1 else v > q66\n",
    "    #     for v, x in [(scores[i], x) for i, x in enumerate(idx)]\n",
    "    # ])\n",
    "\n",
    "    if not as_str:\n",
    "        return idx\n",
    "\n",
    "    # map to class labels\n",
    "    return np.vectorize({0: \"low\", 1: \"mid\", 2: \"high\"}.get)(idx)\n",
    "\n",
    "\n",
    "def quantize_score_df(df_scores, cols, as_str=False):\n",
    "    all_scores = dict()\n",
    "    for col in cols:\n",
    "        all_scores[col] = quantize_scores(df_scores[col], as_str=as_str)\n",
    "    return pd.DataFrame.from_dict(all_scores)\n",
    "\n",
    "\n",
    "as_str = False\n",
    "# y\n",
    "df_study1_scores_cls = quantize_score_df(df_study1_scores, compute.COLS_SCORES, as_str=as_str)\n",
    "df_study2_scores_cls = quantize_score_df(df_study2_scores, compute.COLS_SCORES + compute.COLS_SCORES_S2, as_str=as_str)\n",
    "df_both_scores_cls = quantize_score_df(df_both_scores, compute.COLS_SCORES, as_str=as_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifiers / Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sels_features = [\"DTM\", \"LIWC\", \"LIWC_S\", \"DTM+LIWC\", \"DTM+LIWC_S\"]\n",
    "sels_study = [\"S1\", \"S2\", \"S1+S2\"]\n",
    "sels_var = [\"power\", \"dominance\", \"prestige\", \"power_f\", \"dominance_f\", \"prestige_f\"]\n",
    "\n",
    "\n",
    "def select_data(sel_features, sel_study):\n",
    "    df_dtm = None\n",
    "    if sel_features == \"DTM\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_dtm\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_dtm\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_dtm\n",
    "    elif sel_features == \"LIWC\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_liwc\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_liwc\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_liwc\n",
    "    elif sel_features == \"LIWC_S\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = df_study1_liwc_scaled\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = df_study2_liwc_scaled\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = df_both_liwc_scaled\n",
    "    elif sel_features == \"DTM+LIWC\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = pd.concat([df_study1_dtm, df_study1_liwc], axis=1)\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = pd.concat([df_study2_dtm, df_study2_liwc], axis=1)\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = pd.concat([df_both_dtm, df_both_liwc], axis=1)\n",
    "    elif sel_features == \"DTM+LIWC_S\":\n",
    "        if sel_study == \"S1\":\n",
    "            df_dtm = pd.concat([df_study1_dtm, df_study1_liwc_scaled], axis=1)\n",
    "        elif sel_study == \"S2\":\n",
    "            df_dtm = pd.concat([df_study2_dtm, df_study2_liwc_scaled], axis=1)\n",
    "        elif sel_study == \"S1+S2\":\n",
    "            df_dtm = pd.concat([df_both_dtm, df_both_liwc_scaled], axis=1)\n",
    "    assert df_dtm is not None\n",
    "    return df_dtm\n",
    "\n",
    "\n",
    "def select_scores(sel_study, sel_var):\n",
    "    df_scores = None\n",
    "    if sel_study == \"S1\":\n",
    "        df_scores = df_study1_scores\n",
    "    elif sel_study == \"S2\":\n",
    "        df_scores = df_study2_scores\n",
    "    elif sel_study == \"S1+S2\":\n",
    "        df_scores = df_both_scores\n",
    "    assert df_scores is not None\n",
    "    col = f\"s:{sel_var}\"\n",
    "    assert col in df_scores.columns\n",
    "    return df_scores[col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(X, y, clf, p_grid, num_trials):\n",
    "    scores = np.zeros(num_trials)\n",
    "    params = []\n",
    "\n",
    "    # Loop for each trial\n",
    "    for i in range(num_trials):\n",
    "        # cross-validation techniques for the inner and outer loops\n",
    "        # E.g \"GroupKFold\", \"LeaveOneOut\", \"LeaveOneGroupOut\", etc.\n",
    "        inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "        outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n",
    "\n",
    "        # Non_nested parameter search and scoring\n",
    "        # clf_gs = GridSearchCV(estimator=clf, param_grid=p_grid, cv=outer_cv)\n",
    "        # clf_gs.fit(X, y)\n",
    "        # clf.cv_results_\n",
    "\n",
    "        # Nested CV with parameter optimization\n",
    "        clf_gs = GridSearchCV(estimator=clf, param_grid=p_grid, cv=inner_cv, verbose=0)\n",
    "        #score = cross_val_score(clf_gs, X=X, y=y, cv=outer_cv)\n",
    "\n",
    "        scorer = check_scoring(clf_gs, scoring=None)\n",
    "        cv_results = cross_validate(\n",
    "            estimator=clf_gs,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            scoring={\"score\": scorer},\n",
    "            cv=outer_cv,\n",
    "            return_estimator=True\n",
    "        )\n",
    "        score = cv_results[\"test_score\"]\n",
    "        best_params = cv_results[\"estimator\"][score.argmax()].best_params_\n",
    "\n",
    "        scores[i] = score.mean()\n",
    "        params.append(best_params)\n",
    "\n",
    "    return scores, params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_study = \"S1+S2\"         # one of: S1, S2, S1+S2\n",
    "sel_features = \"DTM+LIWC\"   # one of: DTM, LIWC, LIWC_S, DTM+LIWC, DTM+LIWC_S\n",
    "sel_var = \"power\"           # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "\n",
    "X = select_data(sel_features, sel_study).values\n",
    "y = select_scores(sel_study, sel_var)\n",
    "\n",
    "# convert continuous y variable to class variable\n",
    "#y_cls = y.round()\n",
    "#y_cls = y.astype(int)\n",
    "y_cls = np.vectorize(round)(y)\n",
    "y_lmh = quantize_scores(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'gamma': 0.01}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_23a45_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_23a45\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_23a45_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_23a45_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_23a45_row0_col0\" class=\"data row0 col0\" >0.345000</td>\n",
       "      <td id=\"T_23a45_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_23a45_row1_col0\" class=\"data row1 col0\" >0.345000</td>\n",
       "      <td id=\"T_23a45_row1_col1\" class=\"data row1 col1\" >{'C': 10, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_23a45_row2_col0\" class=\"data row2 col0\" >0.327500</td>\n",
       "      <td id=\"T_23a45_row2_col1\" class=\"data row2 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_23a45_row3_col0\" class=\"data row3 col0\" >0.335000</td>\n",
       "      <td id=\"T_23a45_row3_col1\" class=\"data row3 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_23a45_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_23a45_row4_col0\" class=\"data row4 col0\" >0.357500</td>\n",
       "      <td id=\"T_23a45_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbc9c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"rbf\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "scores, params = run_trials(X, y_cls, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame.from_dict(dict(zip(scores, params)))\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'gamma': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ec9c9_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ec9c9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ec9c9_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_ec9c9_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ec9c9_row0_col0\" class=\"data row0 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ec9c9_row1_col0\" class=\"data row1 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row1_col1\" class=\"data row1 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ec9c9_row2_col0\" class=\"data row2 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row2_col1\" class=\"data row2 col1\" >{'C': 10, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ec9c9_row3_col0\" class=\"data row3 col0\" >0.327500</td>\n",
       "      <td id=\"T_ec9c9_row3_col1\" class=\"data row3 col1\" >{'C': 1, 'gamma': 0.01}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ec9c9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ec9c9_row4_col0\" class=\"data row4 col0\" >0.382500</td>\n",
       "      <td id=\"T_ec9c9_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'gamma': 0.1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad10c070>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"rbf\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 0.01, 'max_iter': 10}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_560fd_row1_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_560fd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_560fd_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_560fd_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_560fd_row0_col0\" class=\"data row0 col0\" >0.320000</td>\n",
       "      <td id=\"T_560fd_row0_col1\" class=\"data row0 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_560fd_row1_col0\" class=\"data row1 col0\" >0.342500</td>\n",
       "      <td id=\"T_560fd_row1_col1\" class=\"data row1 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_560fd_row2_col0\" class=\"data row2 col0\" >0.275000</td>\n",
       "      <td id=\"T_560fd_row2_col1\" class=\"data row2 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_560fd_row3_col0\" class=\"data row3 col0\" >0.282500</td>\n",
       "      <td id=\"T_560fd_row3_col1\" class=\"data row3 col1\" >{'C': 0.01, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_560fd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_560fd_row4_col0\" class=\"data row4 col0\" >0.297500</td>\n",
       "      <td id=\"T_560fd_row4_col1\" class=\"data row4 col1\" >{'C': 0.01, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbd2f40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X, y_cls, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'C': 1, 'max_iter': 10}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9f816_row2_col0, #T_9f816_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9f816\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9f816_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_9f816_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9f816_row0_col0\" class=\"data row0 col0\" >0.370000</td>\n",
       "      <td id=\"T_9f816_row0_col1\" class=\"data row0 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9f816_row1_col0\" class=\"data row1 col0\" >0.355000</td>\n",
       "      <td id=\"T_9f816_row1_col1\" class=\"data row1 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9f816_row2_col0\" class=\"data row2 col0\" >0.415000</td>\n",
       "      <td id=\"T_9f816_row2_col1\" class=\"data row2 col1\" >{'C': 1, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9f816_row3_col0\" class=\"data row3 col0\" >0.397500</td>\n",
       "      <td id=\"T_9f816_row3_col1\" class=\"data row3 col1\" >{'C': 0.1, 'max_iter': 10}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9f816_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9f816_row4_col0\" class=\"data row4 col0\" >0.415000</td>\n",
       "      <td id=\"T_9f816_row4_col1\" class=\"data row4 col1\" >{'C': 1, 'max_iter': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad037970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'alpha': 1, 'max_iter': 5000, 'selection': 'random'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_afa3e_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_afa3e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_afa3e_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_afa3e_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_afa3e_row0_col0\" class=\"data row0 col0\" >-0.005240</td>\n",
       "      <td id=\"T_afa3e_row0_col1\" class=\"data row0 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_afa3e_row1_col0\" class=\"data row1 col0\" >-0.011109</td>\n",
       "      <td id=\"T_afa3e_row1_col1\" class=\"data row1 col1\" >{'alpha': 1, 'max_iter': 1000, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_afa3e_row2_col0\" class=\"data row2 col0\" >-0.011825</td>\n",
       "      <td id=\"T_afa3e_row2_col1\" class=\"data row2 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_afa3e_row3_col0\" class=\"data row3 col0\" >-0.023269</td>\n",
       "      <td id=\"T_afa3e_row3_col1\" class=\"data row3 col1\" >{'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_afa3e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_afa3e_row4_col0\" class=\"data row4 col0\" >0.009863</td>\n",
       "      <td id=\"T_afa3e_row4_col1\" class=\"data row4 col1\" >{'alpha': 1, 'max_iter': 5000, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73ad10c0a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.Lasso()\n",
    "p_grid = {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"random\", \"cyclic\"]}\n",
    "\n",
    "scores, params = run_trials(X, y, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params:  {'alpha': 10, 'max_iter': 250, 'selection': 'random'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7100b_row4_col0 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7100b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7100b_level0_col0\" class=\"col_heading level0 col0\" >scores</th>\n",
       "      <th id=\"T_7100b_level0_col1\" class=\"col_heading level0 col1\" >params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7100b_row0_col0\" class=\"data row0 col0\" >-0.018498</td>\n",
       "      <td id=\"T_7100b_row0_col1\" class=\"data row0 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7100b_row1_col0\" class=\"data row1 col0\" >-0.027791</td>\n",
       "      <td id=\"T_7100b_row1_col1\" class=\"data row1 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7100b_row2_col0\" class=\"data row2 col0\" >-0.016603</td>\n",
       "      <td id=\"T_7100b_row2_col1\" class=\"data row2 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7100b_row3_col0\" class=\"data row3 col0\" >-0.007605</td>\n",
       "      <td id=\"T_7100b_row3_col1\" class=\"data row3 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7100b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7100b_row4_col0\" class=\"data row4 col0\" >-0.001174</td>\n",
       "      <td id=\"T_7100b_row4_col1\" class=\"data row4 col1\" >{'alpha': 10, 'max_iter': 250, 'selection': 'random'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f73acbc63d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this might not be best, we do some linear model but with classes?\n",
    "\n",
    "clf = sklearn.linear_model.Lasso()\n",
    "p_grid = {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"random\", \"cyclic\"]}\n",
    "\n",
    "scores, params = run_trials(X, y_lmh, clf, p_grid, 5)\n",
    "\n",
    "print(\"best params: \", params[scores.argmax()])\n",
    "pd.DataFrame.from_dict(dict(zip(scores, params)))\n",
    "pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"]).style.highlight_max(subset=[\"scores\"], color=\"lightgreen\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test matrix\n",
    "\n",
    "over _Study_, _Variables_, _Features_ and _Estimators_ with Hyper-Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ----------------------------------------\n",
      "# Running for Study: S1 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.026192\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.019900\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ -0.001337 ±0.029382\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.290000 ±0.010296\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.019539 ±0.029860\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.380000 ±0.012083\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.295000 ±0.030757\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.295000 ±0.015297\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.019628 ±0.029884\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.010677\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.300000 ±0.024779\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.305000 ±0.024207\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.260000 ±0.009274\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.011175 ±0.015611\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.280000 ±0.022000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014596\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.315000 ±0.033226\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.270000 ±0.020881\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.275000 ±0.020833\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014599\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.300000 ±0.028107\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.275000 ±0.021307\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.004000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.485000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ -0.003494 ±0.015369\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.033971\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010938 ±0.019912\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.025612\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.016912\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008639 ±0.024615\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.037815\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.010957 ±0.019882\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.032156\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.017029\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008626 ±0.024608\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.027857\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.395000 ±0.033257\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'random'} @ -0.007793 ±0.014259\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.028000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.037940 ±0.033412\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.024819\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.290000 ±0.011225\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002120 ±0.016022\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.026192\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.037729 ±0.033352\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.028566\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.280000 ±0.011402\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ -0.002121 ±0.016009\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.420000 ±0.030757\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.410000 ±0.008124\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006658 ±0.013335\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.019647\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.030430\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.024495\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002467 ±0.016662\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.021213\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.028705\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.355000 ±0.026000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ -0.002461 ±0.016663\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.570000 ±0.004899\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.560000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003361 ±0.020323\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.009274\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099824 ±0.017418\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.017493\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.121973 ±0.011753\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.007348\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099835 ±0.017425\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.015937\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.122015 ±0.011761\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S2 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022045\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.012410\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042992\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.018815\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042253\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.018601\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023875\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003203 ±0.035641\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.016852\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042255\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.016912\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023152\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003216 ±0.035640\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.365000 ±0.015166\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.375000 ±0.019339\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018157 ±0.013030\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.415000 ±0.018276\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.020833\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.370000 ±0.022226\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039193 ±0.010758\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.420000 ±0.016000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018330\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.380000 ±0.022891\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039192 ±0.010754\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.510000 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.019554 ±0.061176\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023958\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.055881 ±0.076484\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.014353\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.034986\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.005339 ±0.066582\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023367\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.055912 ±0.076491\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.020494\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.029428\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.005334 ±0.066585\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.011662\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.470000 ±0.004000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005932 ±0.019277\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.022450\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.006211 ±0.021080\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.019900\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.440000 ±0.027092\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007654 ±0.017074\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.025020\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.006208 ±0.021087\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.018276\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.435000 ±0.020591\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007656 ±0.017062\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.510000 ±0.051536\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.485000 ±0.042544\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001310 ±0.021467\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.445000 ±0.022494\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.005581 ±0.029849\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.530000 ±0.055534\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.445000 ±0.020248\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020825 ±0.025905\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.021213\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.005579 ±0.029832\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.515000 ±0.031401\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.455000 ±0.019494\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020819 ±0.025921\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.655000 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.008781 ±0.009376\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.015033\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010837 ±0.030403\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.520000 ±0.013784\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.000607 ±0.014225\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.014000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010839 ±0.030405\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.530000 ±0.016432\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.000605 ±0.014232\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S1+S2 ...\n",
      "## Running for Variable: ***[[power]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022215\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.367500 ±0.019144\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003178 ±0.006021\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.332500 ±0.021829\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.009811 ±0.010786\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.014832\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.370000 ±0.029326\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.026347 ±0.013033\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.342500 ±0.024829\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.009801 ±0.010763\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.342500 ±0.011979\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.365000 ±0.026907\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.026359 ±0.013023\n",
      "\n",
      "## Running for Variable: ***[[dominance]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.014883\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.347500 ±0.010173\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005442 ±0.009491\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.332500 ±0.009798\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008402\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.360000 ±0.015166\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.340000 ±0.019196\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.013657\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.007829\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.357500 ±0.014457\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.330000 ±0.018615\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "## Running for Variable: ***[[prestige]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.005000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.497500 ±0.002000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.000597 ±0.009936\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.437500 ±0.015297\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.035048 ±0.007926\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.011979\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016275 ±0.010817\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.440000 ±0.015572\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.035040 ±0.007941\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.010886\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016237 ±0.010794\n",
      "\n",
      "## Running for Variable: ***[[power_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.016000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.427500 ±0.002000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001951 ±0.012982\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.011554\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043887 ±0.013905\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.010368\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.385000 ±0.020248\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059043 ±0.012958\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043922 ±0.013916\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.011247\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.390000 ±0.021943\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059042 ±0.012957\n",
      "\n",
      "## Running for Variable: ***[[dominance_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.447500 ±0.016386\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.432500 ±0.013191\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003755 ±0.010660\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.020000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.010381 ±0.021757\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.445000 ±0.015297\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.390000 ±0.019975\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024462 ±0.010246\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.405000 ±0.020000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010431 ±0.021781\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.442500 ±0.013928\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.387500 ±0.013730\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024464 ±0.010393\n",
      "\n",
      "## Running for Variable: ***[[prestige_f]]*** ...\n",
      "### Running on Features: DTM ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.610000 ±0.004472\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.607500 ±0.000000\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001399 ±0.011015\n",
      "\n",
      "### Running on Features: LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.562500 ±0.008216\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.144116 ±0.016951\n",
      "\n",
      "### Running on Features: LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003873\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.577500 ±0.014748\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ 0.092797 ±0.014769\n",
      "\n",
      "### Running on Features: DTM+LIWC ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.572500 ±0.009925\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.144117 ±0.016956\n",
      "\n",
      "### Running on Features: DTM+LIWC_S ...\n",
      "#### Clf: SVC()\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003742\n",
      "#### Clf: LogisticRegression(solver='liblinear')\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.570000 ±0.012981\n",
      "#### Regr: Lasso()\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ 0.092799 ±0.014768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimators and Parameter-Spaces\n",
    "clfs = [\n",
    "    (sklearn.svm.SVC(kernel=\"rbf\"),\n",
    "     {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}),\n",
    "    (sklearn.linear_model.LogisticRegression(solver=\"liblinear\"),\n",
    "     {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}),\n",
    "]\n",
    "regrs = [\n",
    "    (sklearn.linear_model.Lasso(),\n",
    "     {\"alpha\": [10, 1, 0.1, 0.01], \"max_iter\": [250, 1000, 5000], \"selection\": [\"cyclic\", \"random\"]}),\n",
    "]\n",
    "num_runs = 5\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    results.setdefault(sel_study, dict())\n",
    "    print(\"#\", \"-\" * 40)\n",
    "    print(f\"# Running for Study: {sel_study} ...\")\n",
    "    for sel_var in sels_var:\n",
    "        results[sel_study].setdefault(sel_var, dict())\n",
    "        print(f\"## Running for Variable: *** {sel_var} *** ...\")\n",
    "        for sel_features in sels_features:\n",
    "            results[sel_study][sel_var].setdefault(sel_features, dict())\n",
    "            print(f\"### Running on Feature-Set: {sel_features} ...\")\n",
    "\n",
    "            # Select data\n",
    "            X = select_data(sel_features, sel_study).values\n",
    "            y = select_scores(sel_study, sel_var)\n",
    "            y_cls = np.vectorize(round)(y)\n",
    "\n",
    "            # Run classifiers\n",
    "            for clf, p_grid in clfs:\n",
    "                print(f\"#### Clf: {clf}\")\n",
    "                scores, params = run_trials(X, y_cls, clf, p_grid, num_runs)\n",
    "                print(f\"     best params: {params[scores.argmax()]} @ {scores.max():f} ±{scores.std():f}\")\n",
    "                cur_results = pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"])\n",
    "                results[sel_study][sel_var][sel_features][clf.__class__.__name__.rsplit(\".\", 1)[-1]] = cur_results\n",
    "\n",
    "            # Run regressors\n",
    "            for reg, p_grid in regrs:\n",
    "                print(f\"#### Regr: {reg}\")\n",
    "                scores, params = run_trials(X, y, reg, p_grid, num_runs)\n",
    "                print(f\"     best params: {params[scores.argmax()]} @ {scores.max():f} ±{scores.std():f}\")\n",
    "                cur_results = pd.DataFrame(tuple(zip(scores, params)), columns=[\"scores\", \"params\"])\n",
    "                results[sel_study][sel_var][sel_features][reg.__class__.__name__.rsplit(\".\", 1)[-1]] = cur_results\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "fn_results = \"results.pickle\"\n",
    "if not os.path.exists(fn_results):\n",
    "    with open(fn_results, \"wb\") as fp:\n",
    "        pickle.dump(results, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(fn_results, \"rb\") as fp:\n",
    "    results = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ----------------------------------------\n",
      "# Running for Study: S1 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.026192\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.019900\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ -0.001337 ±0.029382\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.290000 ±0.010296\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.019539 ±0.029860\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.380000 ±0.012083\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.295000 ±0.030757\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.002000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.295000 ±0.015297\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.019628 ±0.029884\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.010677\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.300000 ±0.024779\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001635 ±0.028057\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.305000 ±0.024207\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.260000 ±0.009274\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.011175 ±0.015611\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.280000 ±0.022000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014596\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.315000 ±0.033226\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.270000 ±0.020881\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.310000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 100} @ 0.275000 ±0.020833\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.010457 ±0.014599\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.300000 ±0.028107\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.275000 ±0.021307\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.009470 ±0.016074\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.004000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.485000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ -0.003494 ±0.015369\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.033971\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010938 ±0.019912\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.025612\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.016912\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008639 ±0.024615\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.485000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.037815\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.010957 ±0.019882\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.505000 ±0.032156\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.395000 ±0.017029\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.008626 ±0.024608\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.027857\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.395000 ±0.033257\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'random'} @ -0.007793 ±0.014259\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.028000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.037940 ±0.033412\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.024819\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.290000 ±0.011225\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002120 ±0.016022\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.385000 ±0.035553\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.026192\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.037729 ±0.033352\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.380000 ±0.028566\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.280000 ±0.011402\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ -0.002121 ±0.016009\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.420000 ±0.030757\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.410000 ±0.008124\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006658 ±0.013335\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.019647\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.030430\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.024495\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ -0.002467 ±0.016662\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.400000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.350000 ±0.021213\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.006567 ±0.012871\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.028705\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.355000 ±0.026000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ -0.002461 ±0.016663\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.570000 ±0.004899\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.560000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003361 ±0.020323\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.009274\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099824 ±0.017418\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.017493\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.121973 ±0.011753\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.560000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.510000 ±0.007348\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.099835 ±0.017425\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.580000 ±0.009274\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.460000 ±0.015937\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.122015 ±0.011761\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S2 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022045\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.375000 ±0.012410\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042992\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.018815\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042253\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.375000 ±0.018601\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023875\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003203 ±0.035641\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018815\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.325000 ±0.016852\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.004185 ±0.042255\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.016912\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.340000 ±0.023152\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003216 ±0.035640\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.365000 ±0.015166\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.375000 ±0.019339\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018157 ±0.013030\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.415000 ±0.018276\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.375000 ±0.020833\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.370000 ±0.022226\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039193 ±0.010758\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.405000 ±0.034409\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.420000 ±0.016000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.018288 ±0.019953\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.390000 ±0.018330\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.380000 ±0.022891\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.039192 ±0.010754\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.510000 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.019554 ±0.061176\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023958\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.055881 ±0.076484\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.014353\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.034986\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'cyclic'} @ 0.005339 ±0.066582\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.510000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.425000 ±0.023367\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.055912 ±0.076491\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.505000 ±0.020494\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.430000 ±0.029428\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.005334 ±0.066585\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.011662\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.470000 ±0.004000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005932 ±0.019277\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.022450\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.006211 ±0.021080\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.019900\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.440000 ±0.027092\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007654 ±0.017074\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.470000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.430000 ±0.025020\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.006208 ±0.021087\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.470000 ±0.018276\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.435000 ±0.020591\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.007656 ±0.017062\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.510000 ±0.051536\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.485000 ±0.042544\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001310 ±0.021467\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.445000 ±0.022494\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ -0.005581 ±0.029849\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.530000 ±0.055534\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.445000 ±0.020248\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020825 ±0.025905\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.465000 ±0.035014\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.445000 ±0.021213\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ -0.005579 ±0.029832\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.01} @ 0.515000 ±0.031401\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.455000 ±0.019494\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.020819 ±0.025921\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.655000 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.008781 ±0.009376\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.015033\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010837 ±0.030403\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.520000 ±0.013784\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.000607 ±0.014225\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.605000 ±0.014000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ -0.010839 ±0.030405\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.655000 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.530000 ±0.016432\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.000605 ±0.014232\n",
      "\n",
      "# ----------------------------------------\n",
      "# Running for Study: S1+S2 ...\n",
      "## Running for Variable: *** power *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.375000 ±0.022215\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.367500 ±0.019144\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003178 ±0.006021\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.332500 ±0.021829\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.009811 ±0.010786\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.014832\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.370000 ±0.029326\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.026347 ±0.013033\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.357500 ±0.010173\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.342500 ±0.024829\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.009801 ±0.010763\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 100, 'gamma': 0.01} @ 0.342500 ±0.011979\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.365000 ±0.026907\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.026359 ±0.013023\n",
      "\n",
      "## Running for Variable: *** dominance *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.014883\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.347500 ±0.010173\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.005442 ±0.009491\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 100} @ 0.332500 ±0.009798\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008402\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.360000 ±0.015166\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.340000 ±0.019196\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.355000 ±0.016508\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.345000 ±0.013657\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.007829\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.357500 ±0.014457\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.330000 ±0.018615\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.007215 ±0.008464\n",
      "\n",
      "## Running for Variable: *** prestige *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.005000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.497500 ±0.002000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.000597 ±0.009936\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.437500 ±0.015297\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 5000, 'selection': 'random'} @ 0.035048 ±0.007926\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.011979\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016275 ±0.010817\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.497500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.440000 ±0.015572\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.035040 ±0.007941\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.497500 ±0.010886\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.420000 ±0.013874\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.016237 ±0.010794\n",
      "\n",
      "## Running for Variable: *** power_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.016000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.427500 ±0.002000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001951 ±0.012982\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.011554\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043887 ±0.013905\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.010368\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.385000 ±0.020248\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059043 ±0.012958\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.427500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.043922 ±0.013916\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.1} @ 0.432500 ±0.011247\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.390000 ±0.021943\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.059042 ±0.012957\n",
      "\n",
      "## Running for Variable: *** dominance_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.447500 ±0.016386\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.1, 'max_iter': 10} @ 0.432500 ±0.013191\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 250, 'selection': 'cyclic'} @ -0.003755 ±0.010660\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.402500 ±0.020000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 250, 'selection': 'random'} @ 0.010381 ±0.021757\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.445000 ±0.015297\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.390000 ±0.019975\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024462 ±0.010246\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.432500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.405000 ±0.020000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 1, 'max_iter': 1000, 'selection': 'random'} @ 0.010431 ±0.021781\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.442500 ±0.013928\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.387500 ±0.013730\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 5000, 'selection': 'random'} @ 0.024464 ±0.010393\n",
      "\n",
      "## Running for Variable: *** prestige_f *** ...\n",
      "### Running on Feature-Set: DTM ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 10, 'gamma': 0.1} @ 0.610000 ±0.004472\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 1, 'max_iter': 10} @ 0.607500 ±0.000000\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 10, 'max_iter': 250, 'selection': 'cyclic'} @ -0.001399 ±0.011015\n",
      "\n",
      "### Running on Feature-Set: LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.562500 ±0.008216\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 250, 'selection': 'random'} @ 0.144116 ±0.016951\n",
      "\n",
      "### Running on Feature-Set: LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003873\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.577500 ±0.014748\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 5000, 'selection': 'random'} @ 0.092797 ±0.014769\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.607500 ±0.000000\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 100} @ 0.572500 ±0.009925\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.1, 'max_iter': 1000, 'selection': 'random'} @ 0.144117 ±0.016956\n",
      "\n",
      "### Running on Feature-Set: DTM+LIWC_S ...\n",
      "#### Estimator: SVC\n",
      "     best params: {'C': 1, 'gamma': 0.01} @ 0.615000 ±0.003742\n",
      "#### Estimator: LogisticRegression\n",
      "     best params: {'C': 0.01, 'max_iter': 10} @ 0.570000 ±0.012981\n",
      "#### Estimator: Lasso\n",
      "     best params: {'alpha': 0.01, 'max_iter': 1000, 'selection': 'random'} @ 0.092799 ±0.014768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out previous results (simulate matrix run, just print selection and scores/params)\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    print(\"#\", \"-\" * 40)\n",
    "    print(f\"# Running for Study: {sel_study} ...\")\n",
    "    for sel_var in sels_var:\n",
    "        print(f\"## Running for Variable: *** {sel_var} *** ...\")\n",
    "        for sel_features in sels_features:\n",
    "            print(f\"### Running on Feature-Set: {sel_features} ...\")\n",
    "\n",
    "            for est, df in results[sel_study][sel_var][sel_features].items():\n",
    "                print(f\"#### Estimator: {est}\")\n",
    "                scores = df[\"scores\"].values\n",
    "                best_params = df[\"params\"][scores.argmax()]\n",
    "                print(f\"     best params: {best_params} @ {scores.max():f} ±{scores.std():f}\")\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in Data from Study: S1\n",
      "  Best results for >>>power<<< on data: 'LIWC' with 0.374000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.375000 ±0.002000\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'DTM' with 0.262000\n",
      "  -> Clf: SVC with params: {'C': 10, 'gamma': 0.1}; 0.305000 ±0.024207\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'DTM' with 0.485000\n",
      "  -> Clf: LogisticRegression with params: {'C': 0.1, 'max_iter': 10}; 0.485000 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'DTM' with 0.363000\n",
      "  -> Clf: LogisticRegression with params: {'C': 1, 'max_iter': 10}; 0.395000 ±0.033257\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'LIWC' with 0.400000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.400000 ±0.000000\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'LIWC_S' with 0.572000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.580000 ±0.009274\n",
      "\n",
      "Results in Data from Study: S2\n",
      "  Best results for >>>power<<< on data: 'DTM' with 0.361000\n",
      "  -> Clf: LogisticRegression with params: {'C': 1, 'max_iter': 10}; 0.375000 ±0.012410\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'DTM+LIWC' with 0.392000\n",
      "  -> Clf: LogisticRegression with params: {'C': 0.1, 'max_iter': 10}; 0.420000 ±0.016000\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'DTM' with 0.510000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.510000 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'LIWC' with 0.470000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.470000 ±0.000000\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'DTM+LIWC_S' with 0.457000\n",
      "  -> Clf: SVC with params: {'C': 10, 'gamma': 0.01}; 0.515000 ±0.031401\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'DTM' with 0.655000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.655000 ±0.000000\n",
      "\n",
      "Results in Data from Study: S1+S2\n",
      "  Best results for >>>power<<< on data: 'LIWC' with 0.342000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.357500 ±0.010173\n",
      "\n",
      "  Best results for >>>dominance<<< on data: 'LIWC' with 0.340000\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.355000 ±0.016508\n",
      "\n",
      "  Best results for >>>prestige<<< on data: 'LIWC' with 0.497500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.497500 ±0.000000\n",
      "\n",
      "  Best results for >>>power_f<<< on data: 'LIWC' with 0.427500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.427500 ±0.000000\n",
      "\n",
      "  Best results for >>>dominance_f<<< on data: 'LIWC' with 0.432500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.432500 ±0.000000\n",
      "\n",
      "  Best results for >>>prestige_f<<< on data: 'LIWC_S' with 0.612500\n",
      "  -> Clf: SVC with params: {'C': 1, 'gamma': 0.01}; 0.615000 ±0.003873\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out best results for each study-variable\n",
    "# (aggregating over feature and estimator)\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    print(f\"Results in Data from Study: {sel_study}\")\n",
    "\n",
    "    for sel_var in sels_var:\n",
    "        best = None\n",
    "        for sel_features in sels_features:\n",
    "            for clf, df_results in results[sel_study][sel_var][sel_features].items():\n",
    "                scores = df_results[\"scores\"].values\n",
    "                score = scores.mean()\n",
    "                if best is None or score > best[0]:\n",
    "                    best = (score, df_results, clf, sel_features)\n",
    "        print(f\"  Best results for >>>{sel_var}<<< on data: '{best[3]}' with {best[0]:f}\")\n",
    "        scores = best[1][\"scores\"].values\n",
    "        best_params = best[1][\"params\"][scores.argmax()]\n",
    "        print(f\"  -> Clf: {best[2]} with params: {best_params}; {scores.max():f} ±{scores.std():f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize results to excel sheet\n",
    "\n",
    "index = []\n",
    "data = []\n",
    "\n",
    "for sel_study in sels_study:\n",
    "    for sel_var in sels_var:\n",
    "        for sel_features in sels_features:\n",
    "\n",
    "            for est, df in results[sel_study][sel_var][sel_features].items():\n",
    "                index.append([sel_study, sel_var, sel_features, est])\n",
    "                scores = df[\"scores\"].values\n",
    "                best_params = df[\"params\"][scores.argmax()]\n",
    "                data.append(scores.tolist() + [scores.max(), scores.std(), best_params])\n",
    "\n",
    "df_results = pd.DataFrame(data,\n",
    "             columns=[f\"Run {i}\" for i in range(num_runs)] + [\"mean\", \"avg\", \"best estimator params\"],\n",
    "             index=pd.MultiIndex.from_frame(pd.DataFrame(index, columns=[\"Study\", \"Variable\", \"Features\", \"Estimator\"]))).T\n",
    "\n",
    "fn_results_xls = \"results.xlsx\"\n",
    "df_results.to_excel(fn_results_xls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train estimator with 'best' parameters\n",
    "\n",
    "- Run hyper-parameter search with cross-validation &rarr; find best params\n",
    "- Train on one study, then evaluate on other study data - best cross-validation (since unseen)\n",
    "- Compute scores for Accuracy, R²\n",
    "- Extract coefficients of best model (&rarr; what features have highest influence on prediction, based on training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = \"LIWC\"       # one of: DTM, LIWC, LIWC_S, B (DTM, LIWC), B_S (DTM, LIWC_S)\n",
    "sel_var = \"prestige\"    # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "\n",
    "sel_features_train = \"S2\"\n",
    "sel_features_test = \"S1\"\n",
    "\n",
    "X = select_data(sel_data, sel_features_test)\n",
    "X_train = X.values\n",
    "X_labels = X.columns.values\n",
    "y_train = select_scores(sel_features_test, sel_var)\n",
    "y_train_cls = quantize_scores(y_train)\n",
    "\n",
    "X_test = select_data(sel_data, sel_features_test).values\n",
    "y_test = select_scores(sel_features_test, sel_var)\n",
    "y_test_cls = quantize_scores(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'C': 0.01, 'max_iter': 10} with 0.365000 (0.331000 ±0.018815)\n"
     ]
    }
   ],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"linear\")\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.1]}\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegression(solver=\"liblinear\")\n",
    "p_grid = {\"C\": [1, 0.1, 0.01], \"max_iter\": [10, 100, 500]}\n",
    "\n",
    "scores, params = run_trials(X_train, y_train_cls, clf, p_grid, 5)\n",
    "best_params = params[scores.argmax()]\n",
    "\n",
    "print(f\"best params: {best_params} with {scores.max():f} ({scores.mean():f} ±{scores.std():f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cross-validation score: 0.665000\n",
      "\n",
      "R² Score train: -0.04298477702365888, test: -0.04298477702365888\n",
      "Accuracy Score train: 0.665, test: 0.665\n"
     ]
    }
   ],
   "source": [
    "clf.set_params(**best_params)\n",
    "clf.fit(X_train, y_train_cls)\n",
    "score = clf.score(X_test, y_test_cls)\n",
    "print(f\"train cross-validation score: {score:f}\")\n",
    "print()\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# \"overfitting\" on train dataset should often only return 1.0 (max) as score\n",
    "print(f\"R² Score train: {r2_score(y_train_cls, y_train_pred)}, test: {r2_score(y_test_cls, y_test_pred)}\")\n",
    "print(f\"Accuracy Score train: {accuracy_score(y_train_cls, y_train_pred)}, test: {accuracy_score(y_test_cls, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00*'reward' + 0.66*'social' + 0.49*'Quote' + 0.49*'quant' + 0.40*'friend' + 0.37*'posemo' + 0.28*'cause' + 0.27*'prep' + 0.26*'achiev' + -0.42*'percept' + -0.44*'auxverb' + -0.45*'health' + -0.50*'conj' + -0.52*'male' + -0.52*'home' + -0.59*'relig' + -0.65*'adverb' + -0.73*'body' + -1.00*'leisure'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract coefficient, normalize them and filter by importance, then plot\n",
    "\n",
    "coefs = clf.coef_.copy()  # only kernel=\"linear\" in SVC; or LogReg\n",
    "\n",
    "coefs = coefs * np.linspace(-1, 1, len(clf.classes_))[:,np.newaxis]\n",
    "coefs = coefs.sum(axis=0)\n",
    "coefs = compute.normalize_coefs(coefs)\n",
    "\n",
    "values, labels = compute.coef_filter(coefs, X_labels)\n",
    "print(compute.coef_to_human(values, labels))\n",
    "\n",
    "df_coefs = pd.DataFrame(values, index=labels)\n",
    "df_coefs.plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "sel_study = \"S1\"      # one of: S1, S2, S1+S2\n",
    "sel_var = \"power\"     # one of: power, dominance, prestige, power_f, dominance_f, prestige_f\n",
    "                      #         optionally also: workplace_power, workplace_power_f  (only S2)\n",
    "\n",
    "y = select_scores(sel_study, sel_var)\n",
    "\n",
    "# convert continuous y variable to class variable\n",
    "#y_cls = y.round()\n",
    "#y_cls = y.astype(int)\n",
    "y_cls = np.vectorize(round)(y)\n",
    "y_lmh = quantize_scores(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for regression\n",
    "\n",
    "# dummy, baseline predictions\n",
    "y_pred_mean = np.full_like(y, fill_value=np.mean(y))\n",
    "y_pred_median = np.full_like(y, fill_value=np.median(y))\n",
    "y_pred_oov = np.full_like(y, -1)  # ?\n",
    "y_pred_rnd = np.random.rand(*y.shape) * 7\n",
    "\n",
    "# scores\n",
    "s_r2_true = r2_score(y, y)\n",
    "s_r2_mean = r2_score(y, y_pred_mean)\n",
    "s_r2_median = r2_score(y, y_pred_median)\n",
    "s_r2_oov = r2_score(y, y_pred_oov)\n",
    "s_r2_rnd = r2_score(y, y_pred_rnd)\n",
    "\n",
    "s_mse_true = mean_squared_error(y, y)\n",
    "s_mse_mean = mean_squared_error(y, y_pred_mean)\n",
    "s_mse_median = mean_squared_error(y, y_pred_median)\n",
    "s_mse_oov = mean_squared_error(y, y_pred_oov)\n",
    "s_mse_rnd = mean_squared_error(y, y_pred_rnd)\n",
    "\n",
    "# overview table\n",
    "s_reg_df = pd.DataFrame(\n",
    "    [s_r2_true, s_r2_mean, s_r2_median, s_r2_oov, s_r2_rnd,\n",
    "     s_mse_true, s_mse_mean, s_mse_median, s_mse_oov, s_mse_rnd],\n",
    "    columns=[\"score\"],\n",
    "    index=pd.MultiIndex.from_product([[\"r2\", \"mse\"], [\"y_true\", \"mean\", \"median\", \"oov\", \"random\"]], names=[\"Measure\", \"y-values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification\n",
    "\n",
    "# dummy, baseline predictions\n",
    "y_17_pred_mean = np.full_like(y_cls, fill_value=np.mean(y_cls))\n",
    "y_17_pred_median = np.full_like(y_cls, fill_value=np.median(y_cls))\n",
    "y_17_pred_oov = np.full_like(y_cls, -1)  # ?\n",
    "y_17_pred_rnd = np.random.randint(7, size=y_cls.shape)  # or quantize_scores(np.random.rand(*y.shape))\n",
    "\n",
    "y_lmh_pred_mean = np.full_like(y_lmh, fill_value=np.mean(y_lmh))\n",
    "y_lmh_pred_median = np.full_like(y_lmh, fill_value=np.median(y_lmh))\n",
    "y_lmh_pred_oov = np.full_like(y_lmh, -1)  # ?\n",
    "y_lmh_pred_rnd = np.random.randint(3, size=y_lmh.shape)\n",
    "\n",
    "# scores\n",
    "s_17_acc_true = accuracy_score(y_cls, y_cls)\n",
    "s_17_acc_mean = accuracy_score(y_cls, y_17_pred_mean)\n",
    "s_17_acc_median = accuracy_score(y_cls, y_17_pred_median)\n",
    "s_17_acc_oov = accuracy_score(y_cls, y_17_pred_oov)\n",
    "s_17_acc_rnd = accuracy_score(y_cls, y_17_pred_rnd)\n",
    "\n",
    "s_lmh_acc_true = accuracy_score(y_lmh, y_lmh)\n",
    "s_lmh_acc_mean = accuracy_score(y_lmh, y_lmh_pred_mean)\n",
    "s_lmh_acc_median = accuracy_score(y_lmh, y_lmh_pred_median)\n",
    "s_lmh_acc_oov = accuracy_score(y_lmh, y_lmh_pred_oov)\n",
    "s_lmh_acc_rnd = accuracy_score(y_lmh, y_lmh_pred_rnd)\n",
    "\n",
    "# overview table\n",
    "s_clf_df = pd.DataFrame(\n",
    "    [s_17_acc_true, s_17_acc_mean, s_17_acc_median, s_17_acc_oov, s_17_acc_rnd,\n",
    "     s_lmh_acc_true, s_lmh_acc_mean, s_lmh_acc_median, s_lmh_acc_oov, s_lmh_acc_rnd],\n",
    "    columns=[\"accuracy\"],\n",
    "    index=pd.MultiIndex.from_product(\n",
    "        [[\"1..7\", \"low/mid/high\"], [\"y_true\", \"mean\", \"median\", \"oov\", \"rnd\"]],\n",
    "        names=[\"Classes\", \"y-values\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Measure</th>\n",
       "      <th colspan=\"5\" halign=\"left\">r2</th>\n",
       "      <th colspan=\"5\" halign=\"left\">mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y-values</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>random</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.013155</td>\n",
       "      <td>-37.273581</td>\n",
       "      <td>-6.117469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.969716</td>\n",
       "      <td>0.982473</td>\n",
       "      <td>37.114504</td>\n",
       "      <td>6.901923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baselines classification:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Classes</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1..7</th>\n",
       "      <th colspan=\"5\" halign=\"left\">low/mid/high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y-values</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>rnd</th>\n",
       "      <th>y_true</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>oov</th>\n",
       "      <th>rnd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Baselines regression:\")\n",
    "display_html(s_reg_df.T)\n",
    "print(\"Baselines classification:\")\n",
    "display_html(s_clf_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
